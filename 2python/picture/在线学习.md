

https://www.jianshu.com/p/eb5ea922e662?utm_campaign=hugo&utm_medium=reader_share&utm_content=note&utm_source=weixin-timeline&from=timeline&isappinstalled=0



# Online Learning是什么

Online Learning并不是一种模型，而是一种模型的训练方法，Online Learning能够根据线上反馈数据，实时快速地进行模型调整，使得模型及时反映线上的变化，提高线上预测的准确率。

Online  Learning的流程包括：将模型的预测结果展现给用户，然后收集用户的反馈数据，再用来训练模型，形成闭环的系统。

Online Learning训练过程也需要优化一个目标函数，但是Online Learning要求快速求出目标函数的最优解，最好是能有解析解。

## 模型的演化历史

离线打分: 模型离线训练, 对所有⽤户打分, 放在 hbase 等待调⽤ 

离线训练, 在线打分: 模型 T+1 离线 hive 训练. 在线 TF serving, 通过 hbase 查询⽤户特征, 对请求的⽤户打分

近线训练, 在线打分: 模型 T+h 离线 hive 训练. 在线 TF serving, 通过 hbase 查询⽤户特征, 对请求的⽤户打分

在线训练, 在线打分: 模型 T+s 在线实时流训练. 在线 TF serving, 通过 hbase 查询⽤户特征, 对请求的⽤户打分



# FTRL（实现Online Learning算法）

FTL的目标函数一般为求解损失函数最小：
$$
w=argmin w \sum_{i=1}^{t}{fi(w)}
$$
FTRL算法就是在FTL的优化目标的基础上，加入正规化，防止过拟合：
$$
w=argmin w \sum_{i=1}^{t}{fi(w)+R(w)}
$$
但是FTRL算法的损失函数，一般也不是能够很快求解的，这种情况下，一般需要找一个代理的损失函数。

代理损失函数需要满足几个要求：

1. 代理损失函数比较容易求解，最好是有解析解
2. 优化代理损失函数求的解，和优化原函数得到的解差距不能太大

需要引入regret的概念，代理函数*ht*(*wt*)，原函数*ft*(*w*∗)，regret需要满足一定条件，算法才有效

其中w*为上面w，wt 和 regret 为下面两式

​																*wt*=argmin  **w **h**t−1**(*w*)
$$
Regret= \sum_{t=1}^{T}{ft(wt)} - \sum_{t=1}^{T}{ft(w*)}
$$
![1573788942946](C:\Users\zetyun\AppData\Roaming\Typora\typora-user-images\1573788942946.png)

# Online Learning实践

online learning是基于stream的data，无法直接对目标优化，普遍会选择regret作为优化目标

regret与loss最直观的区别就是对wt 的求解是按批次一次计算的，还是根据流数据不断更新的



模型其实就是一系列的参数w，把这些w存在redis里面；online learning就是实时训练、更新redis里面的w；预测时，只需要去redis提取对应的w，然后计算就可以了。我们可能通过参数服务器去实现，我还没太懂。



## 内存节省

预测：

```
L1范数加策略，训练结果w很稀疏，在用w做predict的时候节省了内存
```

训练：

```
在线丢弃训练数据中很少出现的特征（FTRL分开更新的w各维度，每一维不同的步长，per-coordinate）
	1）Poisson Inclusion：对某一维度特征所来的训练样本，以p的概率接受并更新模型；
	2）Bloom Filter Inclusion：用bloom filter从概率上做某一特征出现k次才更新

以上可能是FTRL 本身优化和给我们提供的两种思路，泊松法和布隆过滤器
```

```
浮点数重新编码
1)　　特征权重不需要用32bit或64bit的浮点数存储，存储浪费空间
2)　　16bit encoding，但是要注意处理rounding技术对regret带来的影响
```

```
对训练集的多数负样本进行欠采样
1）CTR远小于50%，所以正样本更加有价值。通过对训练数据集进行subsampling，可以大大减小训练数据集的大小
2）正样本全部采（至少有一个广告被点击的query数据），负样本使用一个比例r采样（完全没有广告被点击的query数据）。但是直接在这种采样上进行训练，会导致比较大的biased prediction
3）解决办法：训练的时候，对样本再乘一个权重。权重直接乘到loss上面，从而梯度也会乘以这个权重。
```



# 过程

实时流样本拼接（特征快照  和 匹配的label口径 通过核销周期进行join）

```
我们⽤的离线数据主要来源于 hive, 且我们的样本产⽣逻辑包括: 
	⊚ 根据特征快照或者独⽴特征表获得特征
	⊚ 根据数据中间层清洗出 label ⼜径
	⊚ 根据核销周期 join 特征和 label
对于 ctr 或者 cvr 的估计, 我们在离线⼀般是使⽤确定的⼜径消费. 在线, 我们则需要时间窗口的概念.
```

数据的消费和反压

```
流数据的消费可以认为是⼀个管道. 每⼀个节点都在消费上⼀个节点产出的数据, 只有当所有的数据流速⼀致的时候, 才会正常.如果有的节点发放速度过快, 消费速度过慢, 则会形成反压; 反之, 则是资源浪费. 并发度的调节, ⾄关重要.
```

回撤补偿

```
如果在时间窗⼜中没有点击, 那么我们怎么去减少影响?
⊚ 回撤机制: 对于已经下发的负样本撤回, 或者进⾏修正
⊚ 补偿机制: 对于已经下发的负样本后⾯来的正样本, 重新下发, 并且对于此类样本补偿权重
```

模型训练	

```
在线模型的训练数据一般是增量数据，进行增量训练
```

validation

```
离线训练,我们会单独保留⼀个测试集.该测试集和训练集独⽴,因此在不存在数据泄漏的情况下,可以做到对于模型的评估.

在线学习, 我们可以采⽤和离线同样的机制.如果我们隔⼀段时间使⽤⼀个 batch 的数据做 validaiton, 在数据本⾝⽐较少的时候, 作出测试集的隔离, 会影响模型的训练效果. 同时, 也没有离线的同⼀个份数据进validation 的效果.如果我们单独使⽤外部测试集进⾏ validation, 则⽆法反映出在线学习对于最新数据的训练效果.

为此, 我们使⽤前向计算作为 validation 的结果计算 auc, 后向计算的求导则继续进⾏. 但同时由于 PS 的机制, 可能⼀个不同的worker 之间的权重不⼀致, 因此未来会考虑单独划分出⼀个worker 进⾏权重的冻结, 专门⽤来测试.
```

样本回流

```
即推荐给用户的数据被曝光后根据用户行为产生的结果再次反馈给模型训练的过程称为样本回流
在线学习一般也不会每一个回流数据立马更新模型，这会导致模型震荡频繁，攒一小段时间是个不错的选择
在实际项目中可能会遇到样本回流不及时的情况，可以设置一个合适的时间窗口，去掉窗口外回流不及时的数据
```

在线部署

```
在线部署的方法众多，具体可以参考https://my.oschina.net/taogang/blog/2222908
```

serving

```
需要根据不同的部署方法暴露出服务接口供其他模块调用
```

