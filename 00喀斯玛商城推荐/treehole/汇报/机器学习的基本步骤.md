# 读入数据

读入用户宽表 商品宽表 行为宽表数据 label表数据



# EDA

探索性数据分析，这一步是为了查看数据的基本情况，保证数据可用，可入模型



如正负样本比例是否均衡

数据是否有缺失值、重复值、单一值

数据类型、数据量，以便能写代码时对spark算子优化

特征是否有相关性等等

通过业务尽量理解每一个字段，可能会有特殊的处理



开始只是一个基本的查看数据情况，在后续数据处理中，也要反复的通过数据探索来决定怎么处理数据



# 特征工程

有时候特征工程之前还有一个数据预处理的过程，包括对一些字符串数据，时间数据和其他数据的简单处理。

特征工程和数据预处理都是对数据的处理，特征工程更偏向于用专门的机器学习方法处理数据，两者并不明确界限，预处理能在数仓完成的尽量在数仓完成。



特征工程根据数据的不同和模型要求的不同，有多种方法，主要目的是为了使数据满足模型对数据格式的要求 以及通过特征工程使模型达到一个更好的效果。



数据采样：调节正负样本比。

单一值、重复值、缺失值、异常值处理。缺失值包括不处理、填充、删除、转化为其他值等方法，异常值有盖帽法，替换填充等。

数值化，也叫编码，包括onehot，labelencod，meanencode，bin-counting等方法，一般针对类别型数据，使数据能被计算，也有使模型更好的提取到数据信息的效果。

离散化，也叫分箱，包括等频，等宽，聚类等方法，目的是使数据更加明显，模型效果更好。

无量纲化，也叫归一化，标准化，对连续数值数据进行，可以加快某些算法的训练速度，统一量纲，可以提升部分模型的精度。

特征衍生，包括 PolynomialFeatures或者通过业务经验构建新特征，也是为了增强模型效果，强力的决定模型的特征一般是衍生出来的。

特征选择，包括Filter、Wrapper、Embedded三大类方法，主要为了筛选出有用的特征，减少模型压力的同时不降低精度。

降维，当不要求解释性和维度太多的时候，可以使用降维算法。

还有一些自定义的方法，如数据变换来改变数据分布，把某些特征的频度作为新的特征等。



特征工程就是反复处理数据，达到使模型能挖掘更多信息，有更好的效果的目的，但是数据本身没有信息的话，模型效果不会因为特征工程提升。



# 模型训练

搜索推荐的排序我们采用 GBDT+LR 模型，是业界广泛使用的机器学习排序模型。

主要原理是通过GBDT组合特征，使用LR模型预测，实际应用中比单独使用两个模型都要好，主要原因是GBDT对特征的处理减少了特征之间的共线性，使得LR模型更好的拟合。



模型训练时的参数确定主要使用网格搜索，交叉验证的方式



可以将数据训练完模型分为训练集和测试集，通过召回率精确率AUC等指标对训练好的模型进行离线评价。



# pipeline + pmml

spark直接部署到线上是非常不安全和不稳定的，

基于我们的几百个特征，数据维度不高的情况下，可以使用spark pipeline + pmml 的形式。

通过java进行线上的模型预测，会损失一些精度。



# 一些解释

通过上边的过程可以看出来，模型训练的过程就是确定参数，确定权重的过程，无法进行人为的干预。



整个过程只是为了排序，如果有需要人工强烈干预的排序，可以放在模型预测前或者预测后，人为的手动进行排序。

比如说，我们觉得某个特征比较重要，可以放在现有es查询中增大权重。

如果是强烈的想要干预排序结果，比如供应商给钱，可以放在人工规则调整这里。



排序也不一定非得针对2000个结果进行排序，只对尽可能准确地结果排序，比如200个或者500个



也可以看出来，无论怎么调整特征工程和模型超参，都没有好的数据重要。模型效果的决定：好的数据>特征工程>模型.





















































