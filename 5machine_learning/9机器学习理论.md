吴恩达2022机器学习：https://www.bilibili.com/video/BV19B4y1W76i

斯坦福大学2014机器学习教程中文笔记目录：http://www.ai-start.com/ml2014/



# 机器学习

定义1：在进行特定编程的情况下，给予计算机学习能力的领域。

定义2：一个程序被认为能从经验**E**中学习，解决任务**T**，达到性能度量值**P**，当且仅当，有了经验**E**后，经过**P**评判，程序在处理T时的性能有所提升。



# 监督学习

回归问题

分类问题



# 无监督学习

聚类算法



# 单变量线性回归

Linear Regression with One Variable



### 模型表示

![image-20220727134254238](picture/image-20220727134254238.png)



### 代价函数

建模误差（modeling error）：模型所预测的值与训练集中实际值之间的差距。

代价(损失)函数：求建模误差的最小时模型参数 的函数，描述经验风险（函数在训练集的**平均损失**）。

目标函数：损失函数+正则项，描述结构风险（过拟合的风险）。

https://www.zhihu.com/question/52398145/answer/209358209



线性回归的代价函数是平方误差代价函数。

![image-20220727135152324](picture/image-20220727135152324.png)



### 梯度下降

梯度下降：一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数的最小值。



梯度下降背后的思想是：开始时我们随机选择一个参数的组合，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到一个局部最小值（**local minimum**），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（**global minimum**），选择不同的初始参数组合，可能会找到不同的局部最小值。



凸函数具有唯一的一个全局最小值。



![image-20220727190217157](picture/image-20220727190217157.png)

w、b 为要计算的参数， α 为步长 ，剩下为 代价函数对参数的求导。当导数为0的时候证明到达了代价函数局部最小点。梯度下降完成。

所有参数同步更新，而不是先更新w，再用更新后的w去计算b。

α 过小，计算很慢，α 过大，可能导致函数不收敛甚至发散，就是直接跨过了函数最低点。

![image-20220801171113352](picture/image-20220801171113352.png)

批量梯度下降：当代价函数使用所有训练数据（批次数据）计算，进行梯度下降 是批量梯度下降。



# 多变量线性回归

### 模型表示

![image-20220728093246113](picture/image-20220728093246113.png)

### 向量运算

矩阵运算是矩阵中的每个元素并行的运算，速度远快于多次的for循环运算。



### 梯度下降

![image-20220801163929134](picture/image-20220801163929134.png)



### 无量纲化

为了不使取值范围过大或过小的X对线性模型造成过大的干扰，可以对变量X进行无量纲化。

无量纲化也可以使梯度下降运行的更加快速，原理是范围过大的x的参数范围很小，在梯度下降的时候容易跨过最低点。



### 梯度下降是否收敛

通过画迭代次数和代价函数的图，可以找出梯度下降收敛的最佳迭代次数。

![image-20220801172806650](picture/image-20220801172806650.png)



### 学习率的选择

代价函数随着迭代次数的增加，反复波动，或者反而一直增大，一般都是因为学习率过大。

![image-20220801184731842](picture/image-20220801184731842.png)

选择合适的学习率：

可以把学习率设置为0.0001，0.001，0.01，0.1等，只迭代几次，观察代价函数的变化。



# 多项式回归

线性回归中都是特征的一次项。

多项式回归中，加入了特征的更高次方（例如平方项或立方项），也相当于增加了模型的自由度，用来捕获数据中非线性的变化。
$$
\hat{h} = \theta_0 + \theta_1 x^1 + \ ... \  + \theta_{n-1} x^{n-1} +  \theta_n x^n
$$


# 逻辑回归

可以实现二分类的算法。

适用于离散化稀疏（onehot）特征、特征之间非线性。适用于高维稀疏特征。



### 模型表示

https://zhuanlan.zhihu.com/p/59137998

https://www.zhihu.com/question/527352472/answer/2434704845



![image-20220802104359451](picture/image-20220802104359451.png)

逻辑回归就是线性回归+sigmoid函数。



### 决策边界

![img](picture/1355387-20180726184054400-1134632155.png)

![img](picture/1355387-20180726190653105-1761881987.png)

每一个轴对应特征x，每条数据特征对应的 y 在空间上分布，可以区分y 不同类别的边界 就是决策边界。

**所谓决策边界就是能够把样本正确分类的一条边界，主要有线性决策边界(linear decision boundaries)和非线性决策边界(non-linear decision boundaries)。**注意：决策边界由参数决定，而不是由数据集的特征决定。



在逻辑回归中，如果概率大于0.5为1类，小于0.5为0类，那么当概率为0.5，即z=0时，得到逻辑回归的决策边界，

决策边界由参数W决定。



关于更多决策边界：https://www.devtalking.com/articles/machine-learning-11/



### 代价函数

采用平方误差作为逻辑回归的代价函数是错误的。

![image-20220802155953402](picture/image-20220802155953402.png)

如果采用平方误差作为逻辑回归的代价函数，则代价函数不是凸函数，采用梯度下降求代价函数最小，会有很多的局部最小值作为干扰。



逻辑回归采用交叉熵损失函数

![math](picture/math.svg)

交叉熵损失推导：

​	1.逻辑回归函数的结果y满足**伯努利分布**：即 y=1的概率 和 y=0的概率 相加为1。

​	2.所以单个样本 y=1的概率可以写成 
$$
y\_pred^y * (1-y\_pred)^(1-y)
$$
​	3.所有样本y = 1 的概率 对上式 概率连乘。就组成一个似然函数。极大似然说明模型拟合的好。**伯努利分布的极大似然估计**

​	4.对极大似然函数取log，即为交叉熵损失。加负号，方便优化。



直观理解：

![image-20220802162258108](picture/image-20220802162258108.png)

![image-20220802162124359](picture/image-20220802162124359.png)

逻辑回归的输出的值在01之间，函数如上图。

可以看到，使用交叉熵损失，y=1时，当逻辑回归的输出值也为1是，损失为0。y=0 时，当逻辑回归的输出值为1时，损失为无限大。

上面两图合并即为交叉熵损失公式。



### 梯度下降

![image-20220802171140729](picture/image-20220802171140729.png)

![image-20220802171543983](picture/image-20220802171543983.png)



# softmax 回归

模型表示

![image-20220819152053889](picture/image-20220819152053889.png)



代价函数

![image-20220819153136279](picture/image-20220819153136279.png)



# 多输出分类

多分类和多输出分类（multi-label classfication）

![image-20220819182200545](picture/image-20220819182200545.png)



# 过拟合

### 什么是过拟合

![image-20220802174103142](picture/image-20220802174103142.png)

欠拟合，模型偏差很高，在训练集上表现不好。

泛化能力强，模型在训练集合测试集上都变现良好。

过拟合，模型方差很好，在训练集上表现良好，在测试集上表现不好。



过拟合和欠拟合也可以从模型容量和数据复杂度之间的关系，欠拟合就是简单模型不能拟合复杂数据。

深度模型，思路就是足够大模型容量下，减少泛化误差。



### 解决过拟合

增加更多的训练数据。

减少特征，尤其是泛化能力弱或者明显不相关的特征。

正则化。



# 正则化

减小参数w的值，以减少不合理特征的特征影响程度。

![image-20220811112714679](picture/image-20220811112714679.png)

目标函数 = 代价函数+正则化项

其中，代价函数保证了训练数据的损失最小，正则化项保证了参数w小，避免了过拟合的风向。

λ平衡代价函数和正则化项，λ越小，正则化作用越小。



正则化之后使用梯度下降求目标函数最小值

![image-20220815110534721](picture/image-20220815110534721.png)

只是梯度下降时关于参数w的导数有变化。同理，逻辑回归的梯度下降也可以使用相同方式推导。



直观理解

![image-20220824134504650](picture/image-20220824134504650.png)

画出代价函数和正则化的等高线图，求目标函数最小，就是求这两图的交点，交点处，目标函数最小。



# 神经网络

起源：模仿大脑的算法。

发展：1950开始有，1980末90初被使用，1990末不再使用，2005再次发展。

领域：语音-->图像-->文本（NLP）--> ......



### 定义

input layer

hidden layer(激活函数)

output layer



神经网络架构：多少个隐藏层，每层多少个神经元（unit）。

下图是2层神经网络，说层数时不包括输入层。

![image-20220818111753135](picture/image-20220818111753135.png)

![image-20220818113346172](picture/image-20220818113346172.png)



forward propagation 正向传播：神经网络每一层从前往后计算。很多情况下，随着层数的增加，神经元减少的结构比较有效。



### tensorflow

基本代码

![image-20220818201333525](picture/image-20220818201333525.png)



计算优化代码，相当于使用sigmoid进行二分类

![image-20220819170645073](picture/image-20220819170645073.png)

Dense Layer：该层神经网络中的每个神经元都会接受所有的输入。



### 计算

代码实现以下3个步骤见上图：

1. 定义神经网络模型
2. 定义损失函数：二分类使用交叉熵损失（BinaryCrossentropy），多分类使用交叉熵损失（categoricalCrossentropy），回归使用平方差损失。
3. 进行训练：使用梯度下降求解损失函数最小值



### 激活函数

sigmoid：

ReLU：g(z) = max(0,z)

线性激活函数 ：g(z) = z

softmax：



激活函数的选择：

二分类 问题，输出层 激活函数通常使用 sigmoid。

多分类 问题，输出层 激活函数通常使用 softmax。

回归 问题，y有负值，输出层 激活函数通常使用 线性激活函数。

回归问题，y只有正值，输出层 激活函数通常使用ReLU。

隐藏层 激活函数 通常使用 ReLU，一般绝对不会使用线性激活函数（会导致神经网络及其简单至失效）。



ReLU 比 sigmoid 更经常使用的原因：

1. ReLU计算复杂性低，速度更快。
2. 在使用梯度下降求损失函数最小时，sigmoid因为两侧都比较平滑，造成梯度下降速度缓慢。



为什么要使用激活函数：如果不使用激活函数，神经网络将变成线性回归函数，不再发挥作用。



### Convolutional Layer

卷积层，每个神经元只接受一部分输入。



Convolutional Layer 优点：

​	1、计算更快。

​	2、减少过拟合。



卷积神经网络：每一层都是卷积层的神经网络。



### Adam

求解代价函数最小值的一个算法。是神经网络中比梯度下降更常用的算法。



直观描述：

当梯度下降方向一直朝一个方向的时候，增大步长α。

当梯度下降方向来回变化的时候，减少步长α。



# 数据挖掘优化

得到更多的数据进行训练。

去掉不关键的特征。寻找新的关键特征。

特征衍生。

增大或者减少正则化系数λ。



https://www.bilibili.com/video/BV19B4y1W76i?p=74&spm_id_from=pageDriver&vd_source=95e4d2371451c9804d5d5d30293ea8c6







































# ----

# 决策树

https://www.mstx.cn/decision-tree/logarithm.html



决策树算法是一种监督学习算法。

- 每个内部节点表示一个属性上的判断
- 每个分支代表一个判断结果的输出
- 每个叶节点代表一种分类结果。
- 根节点包含样本全集



### 概述

构建决策树三个步骤：特征选择、决策树生成、决策树剪枝



##### 特征选择的指标

信息增益，信息增益率，基尼系数，分类错误率。



##### 构造决策树算法

- ID3算法：使用信息增益进行特征选择。
- C4.5算法：使用信息增益率进行特征选择。
- CART算法：CART全称为分类回归树，既可以用于分类，也可以用于回归。



##### 决策树剪枝

为了防止过拟合，可以对决策树进行剪枝。

剪枝分为预剪枝和后剪枝。

预剪枝是在决策树生成过程中，在节点划分前进行估计，如果不能提升，则标记为叶子节点。

后剪枝是决策树生成后，自底向上对非叶子节点进行估计，将子树节点变为叶子节点。



### ID3

##### 信息熵

信息熵香农提出，从物理中熵演化而来。

物理学中“熵”是指物体能量的分布更加均匀，即熵越大，代表状态越混沌。

"信息熵”可以用概率表示，概率越大，越确定，越不混沌，"信息熵”越小。



计算一个事件的信息熵：

一个事件有多个结果，如果这些结果相互独立，则此事件的信息熵为各个事件的不确定性之和，即每个独立事件发生概率的倒数之和，为了满足可加性，使用log

![img](picture/clip_image002.gif)



##### 信息增益

我们使用信息增益来进行特征选择。信息增益越大，说明该特征越能确定结果。

整体数据集的信息熵（I(parent)）一般是保持不变的，信息增益就是（I(parent)）- 当前节点的信息熵。

​    ![img](picture/clip_image002-1642667703064.gif)

信息增益计算示例：

![image-20220120165538960](picture/image-20220120165538960.png)

![image-20220120165545082](picture/image-20220120165545082.png)

##### 构建过程

步骤：根据信息增益来选择进行划分的特征，然后递归地构建决策树。

1. 从根节点开始，计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的划分特征；
2. 由该特征的不同取值建立子节点；
3. 再对子节点递归1-2步，构建决策树；
4. 直到没有特征可以选择或类别完全相同为止，得到最终的决策树。



优缺点：没有剪枝，可能会过度匹配。特征选择使用信息增益，会偏向那些取值较多的特征。



### C4.5

C4.5是对ID3的改进，包括以下方面：

1. 能够完成连续属性的离散化处理。
2. 使用信息增益率来代替信息增益选择属性。
3. 处理缺失值。
4. 剪枝。



##### 信息增益率

信息增益率克服了信息增益偏向选择取值多的属性的不足。

 信息增益率：Gainr(A) = Info(D)-Info_A(D) / H(A) ：其中H(A)为A的熵



其中info名称的熵是通过lable计算的,H(A)的熵是通过特征A计算的。



##### 离散化

连续特征在ID3中有多少个值分裂多少个节点。

离散化的策略就是二分法。连续特征根据选定的父节点分裂值只分裂成两个节点。

父节点分裂值的确定：遍历所有值之间的平均值，看根据平均值进行分裂的两个节点信息增益的和。选择信息增益最大的对应的值作为分裂值。



##### 缺失值的处理

一是在样本某些特征缺失的情况下选择划分的属性。

二是选定了划分属性，对于在该属性上缺失特征的样本的处理。



对于第一个子问题，某一个有缺失特征值的特征A。

C4.5的思路是将数据分成两部分，一部分是有特征值A的数据D1，另一部分是缺失特征A的数据D2。

根据D1数据计算特征A的各种指标，信息增益或者信息增益率。然后乘上一个系数，这个系数是A特征无缺失的样本占总样本的比例。



对于第二个子问题

可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。

即 特征A有3个特征值A1,A2,A3 ，数量为2,3,4，缺失值分别划入A1,A2,A3，划入A1时，缺失值的值是2/9*A1，以此类推。



##### 剪枝处理



### cart

ID3 和 C4.5 只可以处理分类问题，cart 既可以处理分类也可以处理回归。

ID3 和 C4.5 是多叉树，cart 是二叉树。

cart 分类时使用基尼系数，回归时使用平方误差和。



##### 基尼系数

![img](picture/v2-82f818b42b55ff6490bcda9d9cad7182_b.webp)

基尼系数也称为基尼不纯度。基尼系数越小，分类效果越好。Gini(p)反映了从数据集p中随机抽取两个样本，其类别标记不一致的概率。



##### 离散化

cart 不论是对连续特征，还是离散特征，都会进行二分，找到切分点，计算基尼系数。

与C4.5不同的是，用过a属性作为父节点了，子节点仍然可以使用a属性父节点的值进行进一步划分，弥补了二分的的不足。



##### 构建过程

1. 对于当前节点的数据集为D，如果样本个数小于阈值或没有特征，则返回决策子树，当前节点停止递归；
2. 计算样本集D的基尼系数，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归 ；
3. 计算当前节点现有各个特征的各个值的基尼指数，
4. 在计算出来的各个特征的各个值的基尼系数中，选择基尼系数最小的特征A及其对应的取值a作为最优特征和最优切分点。 然后根据最优特征和最优切分点，将本节点的数据集划分成两部分 ![[公式]](https://www.zhihu.com/equation?tex=D_1) 和 ![[公式]](https://www.zhihu.com/equation?tex=D_2) ，同时生成当前节点的两个子节点，左节点的数据集为 ![[公式]](https://www.zhihu.com/equation?tex=D_1) ，右节点的数据集为 ![[公式]](https://www.zhihu.com/equation?tex=D_2) 。
5. 对左右的子节点递归调用1-4步，生成CART分类树；



##### 剪枝处理

在分类回归树中可以使用的后剪枝方法有多种，比如：代价复杂性剪枝、最小误差剪枝、悲观误差剪枝等。

代价复杂性剪枝: https://www.bilibili.com/read/cv11066239/



##### 特征重要性

两个相同的特征，重要性会是最重要的两个吗？

某个特征，在一个节点二分后，子节点不在使用此特征，孙子节点会再次使用此特征分裂吗？



# GBDT

构建CART回归树，使用平方损失，划分节点。y是真实值，c是节点平均值。

![preview](picture/bVbi3i6)

对于平方损失来说，残差 = 真实值 - 预测值。对于其他损失，可以使用 损失函数的偏导 作为残差。

```
D1=训练集D
for i range m:									// m为回归树数量
  Ti=根据Di使用训练出的一棵CART
  使用Ti对Di进行预测，rj=Ti对Di第j个样例的预测结果，并获取每个训练样本的残差yj'， 其中yj'=yj-rj
  Di+1={(x1,y1'),(x2,y2'),...,(xn,yn')}

// 最后的模型为：
f(x)=T1+T2+...+Tm
```



XGBOOST

1. GBDT以传统CART作为基分类器，而xgboost支持线性分类器。
2. GBDT在优化时只用到一阶导数，xgboost对代价函数做了二阶Talor展开。
3. 自动处理缺失值，自动学习分裂方向。
4. 支持列抽样，防止过拟合。
5. 引入正则化项。
6. 对特征的值排序是并行的。





#原理
随机森林抽取数据采取的是自助采样法（即有放回的抽取数据）
随机森林分类最后结果的确定是通过软投票（每个分类器出所有类的概率，计算每个类的概率的加权平均值，取最大的类），而不是硬投票（每个分类器一个标签，最后结果是最多的）





[PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能](https://link.zhihu.com/?target=http%3A//www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html)



有些算法本身是二分类算法，如逻辑回归，adaboost ，但是sklearn都给他们封装成了可以进行多分类、排序 的开箱即用的算法。写在原理那





所有**线性模型** 都假设特征之间是相互独立的。



奥卡姆剃刀原则的原理。
	在模型选择的时候，需要优先选择较为简单的模型(相同的泛化误差的情况下)





距离计算



什么是分布不一致

https://mp.weixin.qq.com/s/aX1EwzV56bIgCCZgiKTB7g





























