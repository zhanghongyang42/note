交叉验证多次划分训练集和验证集。

超参数搜索，训练多组超参数。

sklearn中超参数搜索可以包括交叉验证。



# 交叉验证

```python
#原因
**训练集**训练好模型后直接用**验证集**进行预测，可能过拟合（超参数对某个验证集拟合的好），所以需要把训练集分为训练集和**验证集**。
但是这就减少了训练数据，而且验证结果受数据集划分影响。
所以要使用交叉验证（多次划分训练集和验证集）。

#作用：
1.可以在划分训练集和验证集过程中使用**全量的数据**进行训练。
2.得到模型的验证**平均评分**（各种指标）。
3.验证模型（超参数）稳定性。
```



```python
#k折交叉验证、分层k折(用于数据不平衡)
from sklearn.model_selection import KFold,StratifiedKFold
from sklearn.datasets import load_iris
from sklearn import svm
from sklearn.model_selection import cross_validate

iris = load_iris()

clf = svm.SVC(kernel='linear', C=1)
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)
cv = KFold(n_splits=4, shuffle=True, random_state=42)

scores = cross_validate(clf, iris.data, iris.target, cv=5, scoring='f1_macro',return_estimator=True)

print(scores.keys())

print(scores['fit_time'])
print(scores['score_time'])
print(scores['estimator'])
print('Accuracy : %0.2f'%scores['test_score'].mean())
```

```python
#自定义交叉验证
from sklearn.datasets import load_iris
from sklearn import svm
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import cross_validate

iris = load_iris()

clf = svm.SVC(kernel='linear', C=1)

cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)
print(cross_validate(clf, iris.data, iris.target, cv=cv))
```

```python
#时间序列数据交叉验证，数据需按照时间排列好
#原理是拿出前一部分数据，用后一部分验证。看结果就懂了
import numpy as np
from sklearn.model_selection import TimeSeriesSplit

X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([1, 2, 3, 4, 5, 6])
tscv = TimeSeriesSplit(n_splits=3)

for train, test in tscv.split(X,y):
     print("%s %s" % (train, test))
```



# 网格搜索

```python
from sklearn.datasets import load_iris
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, KFold

iris = load_iris()
X_iris = iris.data
y_iris = iris.target

p_grid = {"C": [1, 10, 100],"gamma": [.01, .1]}
svm = SVC(kernel="rbf")
cv = KFold(n_splits=4, shuffle=True, random_state=i)
 
clf = GridSearchCV(estimator=svm, param_grid=p_grid, cv=cv)
clf.fit(X_iris, y_iris)

print(clf.best_score_)
print(clf.best_params_)
clf = clf.best_estimator_
```



# 随机搜索

相比于网格搜索，相当于用一点准确性换来了大幅的性能提升。

```python
from scipy.stats import randint as sp_randint
from sklearn.model_selection import RandomizedSearchCV
from sklearn.datasets import load_digits
from sklearn.ensemble import RandomForestClassifier

digits = load_digits()
X, y = digits.data, digits.target

n_iter_search = 20
clf = RandomForestClassifier(n_estimators=20)
param_dist = {"max_depth": [3, None],"max_features": sp_randint(1, 11),
              "min_samples_split": sp_randint(2, 11),
              "bootstrap": [True, False],"criterion": ["gini", "entropy"]}


random_search = RandomizedSearchCV(clf, param_distributions=param_dist,n_iter=n_iter_search, 
                                   cv=5, iid=False)
random_search.fit(X, y)
```



