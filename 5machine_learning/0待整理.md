# 提前注意

大数据量的特征工程比较容易，小数据量的特征工程的很多操作容易对结果造成负面影响。

 **数值型**和**类别型**，数值型又有 **连续型**和**离散型**。



训练集和测试集同处理

```
1.批量任务时：训练集和测试集可以合并处理，但是处理完后要可以像以前一样分开，优先

2.实时任务时：训练集和测试集数据满足‘独立同分布’,即可同步处理
	2.1测试集出现了训练集没有出现的极大极小值，离散化可以将上下限放开，归一化只能先忽略这个影响
	2.2测试集出现了训练集没有的类别变量，sklearn的onehot有一个忽略参数
```



避免时间穿越和数据泄露的问题。





# 模型评价

1.模型自带score方法

2.sklearn.metrics的指标：`_score` 结尾返回值越高越好，`_error` 或 `_loss` 结尾返回值越低越好

3.网格搜索和交叉验证的评分方法：可以同时进行多指标评价



### 分类指标

其中一些仅限于二分类示例:

| 调用                                                         | 功能                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [`precision_recall_curve`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve)(y_true, probas_pred) | Compute precision-recall pairs for different probability thresholds |
| [`roc_curve`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve)(y_true, y_score[, pos_label, …]) | Compute Receiver operating characteristic (ROC)              |

其他也可以在多分类示例中运行:

| 调用                                                         | 功能                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [`cohen_kappa_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html#sklearn.metrics.cohen_kappa_score)(y1, y2[, labels, weights, …]) | Cohen’s kappa: a statistic that measures inter-annotator agreement. |
| [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix)(y_true, y_pred[, labels, …]) | Compute confusion matrix to evaluate the accuracy of a classification |
| [`hinge_loss`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html#sklearn.metrics.hinge_loss)(y_true, pred_decision[, labels, …]) | Average hinge loss (non-regularized)                         |
| [`matthews_corrcoef`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html#sklearn.metrics.matthews_corrcoef)(y_true, y_pred[, …]) | Compute the Matthews correlation coefficient (MCC)           |

有些还可以在 multilabel case （多重示例）中工作:

| 调用                                                         | 功能                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [`accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)(y_true, y_pred[, normalize, …]) | Accuracy classification score.                               |
| [`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report)(y_true, y_pred[, …]) | Build a text report showing the main classification metrics  |
| [`f1_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score)(y_true, y_pred[, labels, …]) | Compute the F1 score, also known as balanced F-score or F-measure |
| [`fbeta_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score)(y_true, y_pred, beta[, labels, …]) | Compute the F-beta score                                     |
| [`hamming_loss`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html#sklearn.metrics.hamming_loss)(y_true, y_pred[, labels, …]) | Compute the average Hamming loss.                            |
| [`jaccard_similarity_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_similarity_score.html#sklearn.metrics.jaccard_similarity_score)(y_true, y_pred[, …]) | Jaccard similarity coefficient score                         |
| [`log_loss`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss)(y_true, y_pred[, eps, normalize, …]) | Log loss, aka logistic loss or cross-entropy loss.           |
| [`precision_recall_fscore_support`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support)(y_true, y_pred) | Compute precision, recall, F-measure and support for each class |
| [`precision_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score)(y_true, y_pred[, labels, …]) | Compute the precision                                        |
| [`recall_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score)(y_true, y_pred[, labels, …]) | Compute the recall                                           |
| [`zero_one_loss`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.zero_one_loss.html#sklearn.metrics.zero_one_loss)(y_true, y_pred[, normalize, …]) | Zero-one classification loss.                                |

一些通常用于 ranking:

| 调用                                                         | 功能                                                    |
| ------------------------------------------------------------ | ------------------------------------------------------- |
| [`dcg_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.dcg_score.html#sklearn.metrics.dcg_score)(y_true, y_score[, k]) | Discounted cumulative gain (DCG) at rank K.             |
| [`ndcg_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ndcg_score.html#sklearn.metrics.ndcg_score)(y_true, y_score[, k]) | Normalized discounted cumulative gain (NDCG) at rank K. |

有些工作与 binary 和 multilabel （但不是多类）的问题:

| 调用                                                         | 功能                                                      |
| ------------------------------------------------------------ | --------------------------------------------------------- |
| [`average_precision_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score)(y_true, y_score[, …]) | Compute average precision (AP) from prediction scores     |
| [`roc_auc_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score)(y_true, y_score[, average, …]) | Compute Area Under the Curve (AUC) from prediction scores |



##### 计算KS

```python
y_predict_proba = est.predict_proba(X_test)[:,1]
fpr,tpr,thresholds= sklearn.metrics.roc_curve(np.array(Y_test),y_predict_proba)
print ('KS:',max(tpr-fpr))
```



##### 其他

*Cohen’s kappa*

```python
#二分类，多分类
#用于比较不同人工标注的准确性，0.8以上说明两个人标注的都准确
from sklearn.metrics import cohen_kappa_score
y_true = [2, 0, 2, 2, 0, 1]
y_pred = [0, 0, 2, 2, 0, 2]
cohen_kappa_score(y_true, y_pred)
```



Jaccard 距离计算

```python
import numpy as np
from sklearn.metrics import jaccard_score

y_true = np.array([[0, 1, 1],[1, 1, 0]])
y_pred = np.array([[1, 1, 1],[1, 0, 0]])

jaccard_score(y_true[0], y_pred[0])  
```



log损失

```python
#logistic 回归损失  或者  交叉熵损失
from sklearn.metrics import log_loss

y_true = [0, 0, 1, 1]
y_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]]

log_loss(y_true, y_pred)  
```





### 多标签评价

多标签的排序评价   这部分官网看不懂

所有涉及到多标签，多输出的都还没搞太懂





### 网格搜索评价

#### 现有的*网格得分*函数

| Scoring（得分）                | Function（函数）                                             | Comment（注解）                                              |
| ------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **Classification（分类）**     |                                                              |                                                              |
| ‘accuracy’                     | [`metrics.accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score) |                                                              |
| ‘average_precision’            | [`metrics.average_precision_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score) |                                                              |
| ‘f1’                           | [`metrics.f1_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) | for binary targets（用于二进制目标）                         |
| ‘f1_micro’                     | [`metrics.f1_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) | micro-averaged（微平均）                                     |
| ‘f1_macro’                     | [`metrics.f1_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) | macro-averaged（宏平均）                                     |
| ‘f1_weighted’                  | [`metrics.f1_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) | weighted average（加权平均）                                 |
| ‘f1_samples’                   | [`metrics.f1_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) | by multilabel sample（通过 multilabel 样本）                 |
| ‘neg_log_loss’                 | [`metrics.log_loss`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss) | requires `predict_proba` support（需要 `predict_proba` 支持） |
| ‘precision’ etc.               | [`metrics.precision_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score) | suffixes apply as with ‘f1’（后缀适用于 ‘f1’）               |
| ‘recall’ etc.                  | [`metrics.recall_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score) | suffixes apply as with ‘f1’（后缀适用于 ‘f1’）               |
| ‘roc_auc’                      | [`metrics.roc_auc_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) |                                                              |
| **Clustering（聚类）**         |                                                              |                                                              |
| ‘adjusted_mutual_info_score’   | [`metrics.adjusted_mutual_info_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html#sklearn.metrics.adjusted_mutual_info_score) |                                                              |
| ‘adjusted_rand_score’          | [`metrics.adjusted_rand_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html#sklearn.metrics.adjusted_rand_score) |                                                              |
| ‘completeness_score’           | [`metrics.completeness_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score) |                                                              |
| ‘fowlkes_mallows_score’        | [`metrics.fowlkes_mallows_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fowlkes_mallows_score.html#sklearn.metrics.fowlkes_mallows_score) |                                                              |
| ‘homogeneity_score’            | [`metrics.homogeneity_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score) |                                                              |
| ‘mutual_info_score’            | [`metrics.mutual_info_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mutual_info_score.html#sklearn.metrics.mutual_info_score) |                                                              |
| ‘normalized_mutual_info_score’ | [`metrics.normalized_mutual_info_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html#sklearn.metrics.normalized_mutual_info_score) |                                                              |
| ‘v_measure_score’              | [`metrics.v_measure_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html#sklearn.metrics.v_measure_score) |                                                              |
| **Regression（回归）**         |                                                              |                                                              |
| ‘explained_variance’           | [`metrics.explained_variance_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score) |                                                              |
| ‘neg_mean_absolute_error’      | [`metrics.mean_absolute_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error) |                                                              |
| ‘neg_mean_squared_error’       | [`metrics.mean_squared_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error) |                                                              |
| ‘neg_mean_squared_log_error’   | [`metrics.mean_squared_log_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_log_error.html#sklearn.metrics.mean_squared_log_error) |                                                              |
| ‘neg_median_absolute_error’    | [`metrics.median_absolute_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.median_absolute_error.html#sklearn.metrics.median_absolute_error) |                                                              |
| ‘r2’                           | [`metrics.r2_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score) |                                                              |

```python
from sklearn import svm, datasets
from sklearn.model_selection import cross_val_score

iris = datasets.load_iris()
X, y = iris.data, iris.target
clf = svm.SVC(probability=True, random_state=0)
cross_val_score(clf, X, y, scoring='neg_log_loss')
```



#### 构造*网格得分*函数

##### 从[`sklearn.metrics`](https://sklearn.apachecn.org/docs/master/classes.html#module-sklearn.metrics)构造

因为有些评价指标有参数，所以需要自行构造

```
from sklearn.metrics import fbeta_score, make_scorer
from sklearn.model_selection import GridSearchCV
from sklearn.svm import LinearSVC

ftwo_scorer = make_scorer(fbeta_score, beta=2)

grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]}, scoring=ftwo_scorer)
```



##### 自定义评价函数 构造

```python
import numpy as np
from sklearn.metrics import make_scorer
from sklearn.dummy import DummyClassifier

#构建了一个评分函数
def my_custom_loss_func(y_true, y_pred):
    diff = np.abs(y_true - y_pred).max()
    return np.log1p(diff)

#数据集
X = [[1], [1]]
y = [0, 1]

#训练模型
clf = DummyClassifier(strategy='most_frequent', random_state=0)
clf = clf.fit(X, y)

#进行评价
print(my_custom_loss_func(clf.predict(X), y))

#构造网格得分函数
score = make_scorer(my_custom_loss_func, greater_is_better=False)
#greater_is_better返回的是损失得分，即函数返回值的相反数
print(score(clf, X, y))
```



### 业务指标

营销响应业务评价/topN指标

需要修改

```python
import numpy as np
import pandas as pd
from scipy.stats import scoreatpercentile
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_curve, auc

#营销响应业务评价
#因为是按概率分组，相比通用二分类评价，本组总数和预测为1的数是一个，本组预测1实际1的数和实际1的数是一个
def evalua(y_true,y_prob):
    y_true = np.array(y_true)
    y_prob = y_prob[:,1]
    result = pd.DataFrame()
    result['target'] = y_true
    result['proba'] = y_prob
    
    grade = result['proba'].copy()
    for i in range(10):
        point1 = scoreatpercentile(result.proba, i*(100/10))
        point2 = scoreatpercentile(result.proba, (i+1)*(100/10))
        grade[(result['proba'] >= point1) & (result['proba'] <= point2)] = (10-i)
    result['grade'] = grade
    
    #原始数据
    #根据预测为1的概率分组
    #每组人数
    #每组实际为1的人数
    #总人数
    #总实际为1的人数
    
    #数据处理表process
    process = result.groupby(by=['grade']).agg({'target':'sum','proba':'count'})
    process.reset_index(inplace=True)                                                           # grade作为分位点
    process.rename(columns={'target':'response_count','proba':'num_count'},inplace=True)
    process[['cumsum_response_count','cumsum_num_count']] = process[['response_count','num_count']].cumsum(axis=0)     #计算累计数量
    del result
     
    #总体表table
        #表头
    table = pd.DataFrame(columns=['分位点','客户数','购买人数','响应率','覆盖率','提升度','累计购买人数','累计总人数','累计响应率','累计覆盖率','累计提升度'])
        #填表
    table['分位点'] = process['grade'] 

    table['客户数'] = process['num_count']

    table['购买人数'] = process['response_count']

    table['累计购买人数'] = process['cumsum_response_count']

    table['累计总人数'] = process['cumsum_num_count']

    table['响应率'] = table['购买人数']/table['客户数']
    table['响应率'] = round(table['响应率'], 4)           #保留4位小数

    table['覆盖率'] = table['购买人数']/table['购买人数'].sum()
    table['覆盖率'] = round(table['覆盖率'], 4)   

    table['提升度'] = table['响应率']/(sum(table['购买人数'])/sum(table['客户数']))
    table['提升度'] = round(table['提升度'], 2)   

    table['累计响应率'] = table['累计购买人数']/table['累计总人数']
    table['累计响应率'] = round(table['累计响应率'], 4)   

    table['累计覆盖率'] = table['累计购买人数']/sum(table['购买人数'])
    table['累计覆盖率'] = round(table['累计覆盖率'], 4)  

    table['累计提升度'] = table['累计响应率']/(sum(table['购买人数'])/sum(table['客户数']))
    table['累计提升度'] = round(table['累计提升度'], 2)  

    print(table)
    del process
    
    #累计响应率绘制
    from matplotlib.ticker import FuncFormatter

    def to_percent(temp,position):
        return '%1.0f'%(100*temp) + '%'
    def to_percent2(temp,position):
        return '%1.0f'%(10*temp) + '%'
 
    plt.plot(table['分位点'],table['累计响应率'], color='red')            #模型组绘制
    plt.plot(table['分位点'],[sum(table['购买人数'])/sum(table['客户数'])]*10,color='blue')   #随机组绘制

    for xy in table['累计响应率'].reset_index().values:
        plt.annotate("%s%%" % round(xy[1]*100,2), xy=xy, xytext=(-15, 20), textcoords='offset points')
    
    plt.xticks([1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0])

    plt.gca().xaxis.set_major_formatter(FuncFormatter(to_percent2))
    plt.gca().yaxis.set_major_formatter(FuncFormatter(to_percent))

    plt.title('Cumulative_response_rate')
    plt.ylabel('response_rate')
    plt.legend(["model","random"]) 

    plt.show()
    
    #累计覆盖率绘制
    plt.plot(table['分位点'],table['累计覆盖率'], color='red')  
    plt.plot(table['分位点'],[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1], color='blue')

    for xy in table['累计覆盖率'].reset_index().values:
        plt.annotate("%s%%" % round(xy[1]*100,2), xy=xy, xytext=(-5, 5), textcoords='offset points')

    plt.xticks([1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0])

    plt.gca().xaxis.set_major_formatter(FuncFormatter(to_percent2))
    plt.gca().yaxis.set_major_formatter(FuncFormatter(to_percent))

    plt.title('Cumulative_coverage')
    plt.ylabel('coverage')
    plt.legend(["model","random"])

    plt.show()
    
    #累计提升度绘制
    # import scikitplot as skplt
    # skplt.metrics.plot_lift_curve(result['target'],result['proba'])

    plt.plot(table['分位点'],table['累计提升度'], color='red')
    plt.plot(table['分位点'],[1]*10, color='blue')

    for xy in table['累计提升度'].reset_index().values:
        plt.annotate("%s" % round(xy[1],2), xy=xy, xytext=(25, 10), textcoords='offset points') 
    
    plt.xticks([1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0])
    plt.gca().xaxis.set_major_formatter(FuncFormatter(to_percent2))

    plt.title('Cumulative_lift')
    plt.ylabel('lift')
    plt.legend(["model","random"])

    plt.show()
evalua(y_true,y_prob)
```



# 模型持久化

### pickle

python中几乎所有的数据类型（列表，字典，集合，类等）都可以用  pickle  来序列化

```python
import pickle

with open('svm.pickle','wb') as fw:
pickle.dump(svm,fw)				#文件保存
with open('svm.pickle', 'rb') as fr:
svm = pickle.load(fr)		#文件加载

s = pickle.dumps(clf)			#内存保存
clf = pickle.loads(s)			#内存加载
```



### joblib

一般模型都可以用  joblib  来序列化

```python
from sklearn.externals import joblib

joblib.dump(clf, 'filename.pkl')
clf = joblib.load('filename.pkl')
```



### mleap

https://www.dazhuanlan.com/2019/10/02/5d93f8c1667a1/

https://mleap-docs.combust.ml/



# 模型部署

模型部署一般就是把训练的模型持久化，然后运行服务器加载模型，并提供REST或其它形式的服务接口。

https://my.oschina.net/taogang/blog/2222908



# 调优

最重要的是数据 和 你定义的问题的合理性，保证这两点。



### 最优模型阈值

可以通过调节阈值调整 召回率和准确率

```python
y_prob = pd.DataFrame(clf.predict_proba(X_test)[:,1])

from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_auc_score

for i in range(9):
    i = (i+1)/10
    y_pred = y_prob.applymap(lambda x: 1 if x>=i else 0)
    print(classification_report(y_test, y_pred))
    print(confusion_matrix(y_test, y_pred))
    print(roc_auc_score(y_test, y_pred))
    print('---------------------------------------------------')
```



### 模型过拟合

过拟合即模型在训练集上表现良好，在测试集上表现很差。



##### Early stopping

##### 正则化

##### 数据集扩增

##### DropOut

##### 切分数据

拿出数据的一部分进行训练，这样可能会减轻过拟合，但也可能使过拟合的程度加大



### 内存优化

解决因为内存不足进行的代码层面的优化



##### 删变量

删掉不用的变量，尤其是表

```python
del df
```



##### 读pickl

多次重复读取大数据，把读取的数据保存为pickl或者hdf，会比读取csv快很多

```python
df.to_hdf('foo.h5','df')
pd.read_hdf('foo.h5','df')
```



##### 最优数据量

多次运行的场景可以 选出最优数据量

```python
from sklearn.model_selection import learning_curve
from sklearn.svm import SVC

train_sizes, train_scores, valid_scores = learning_curve(SVC(kernel='linear'), X, y, 
                                                         train_sizes=[50, 80, 110], cv=5)
```



##### 改变数据类型

加快了读取速度，不重新读取的话只用中间一部分代码就好

```python
import pandas as pd

#读入数据，查看数据类型
df = pd.read_csv('train.csv')
print(df.dtypes)

#对不同的数据类型进行不同的优化
for col in df.select_dtypes('object'):
    if len(df[col].unique())/len(df[col])<0.5:
        df[col] = df[col].astype('category')

aa = df.select_dtypes('int64').apply(pd.to_numeric,downcast='signed')
bb = df.select_dtypes('float').apply(pd.to_numeric,downcast='float')

#将优化后的数据类型存到字典中
dic = {}
for i,j in zip(df.dtypes,df.dtypes.index):
    dic[j] = i
for i,j in zip(aa.dtypes,aa.dtypes.index):
    dic[j] = i
for i,j in zip(bb.dtypes,bb.dtypes.index):
    dic[j] = i

#重新读取数据
del df,aa,bb
df = pd.read_csv('train.csv',dtype=dic)
```



### 不平衡处理

一般正负样本比大于1：10，即认为不均衡。均衡数据可以不处理。



##### 模型调整

调整分类阈值，默认阈值的0.5。移动分类阈值也是常见的模型调优的方法。

一些模型也自带参数可以调整分类权重。

树模型对数据不平衡的敏感性较低。



##### 数据采样

采样方法分为  过采样 和  欠采样。数据足够的话，优先欠采样。

一般只对训练集进行 采样，测试集不需要。





# 示例

### Titanic示例

这个示例手动的进行了模型的融合

```python
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import pandas_profiling
from sklearn.model_selection import KFold
from sklearn.preprocessing import OneHotEncoder
pd.options.display.max_columns = None

#读入数据
train_data = pd.read_csv("train.csv")
test_data = pd.read_csv("test.csv")

#查看数据是否平衡
print(train_data["Survived"].value_counts())

#查看数值型还是类别型
print(train_data.dtypes)

#查看缺失值，结合分析出异常值，0值，查看高基数特征，整体相关性
# if __name__ == '__main__':
#     pfr = pandas_profiling.ProfileReport(train_data)
#     pfr.to_file("./example_train.html")
#     pfr = pandas_profiling.ProfileReport(test_data)
#     pfr.to_file("./example_test.html")


#缺失值处理
train_data = train_data.drop(['Cabin'],axis=1)

#数值化
train_data = train_data.drop(['Name','Ticket'],axis=1)
train_onehot = pd.get_dummies(train_data[['Sex','Embarked']])

train_data = pd.concat([train_data,train_onehot],axis=1)
train_data = train_data.drop(['Sex','Embarked'],axis=1)


#只对数值型起作用，0.1以下认为没有相关性，0.5以上认为强相关
correlations = train_data.corr()['Survived'].sort_values()
print(correlations)
#进一步查看与label的相关性
def kde_target(df, lable, var_name):
    plt.figure(figsize = (12, 6))
    sns.kdeplot(df.ix[df[lable] == 0, var_name], label = '0')
    sns.kdeplot(df.ix[df[lable] == 1, var_name], label = '1')
    plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name)
    plt.legend()#加上图例
    plt.show()
kde_target(train_data,"Survived","Parch")
kde_target(train_data,"Survived","SibSp")
kde_target(train_data,"Survived","PassengerId")
kde_target(train_data,"Survived","Age")
kde_target(train_data,"Survived","Embarked_Q")

train_data = train_data.drop(['Age','Embarked_Q','PassengerId'],axis=1)

#删除线性相关变量，看热力图
def test(df):
    dfData = df.corr()
    plt.subplots(figsize=(9, 9)) # 设置画面大小
    sns.heatmap(dfData, annot=True, vmax=1, square=True, cmap="Blues")
    plt.show()
test(train_data)

#选出x和y
x = train_data.drop(["Survived"],axis=1)
y = train_data['Survived']

#切分训练集和测试集
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2,random_state=42)

#模型融合
    # 导入一级模型
from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier,GradientBoostingClassifier, ExtraTreesClassifier)
import xgboost as xgb
from sklearn.svm import SVC
rf_model = RandomForestClassifier(random_state=42)
adb_model = AdaBoostClassifier(random_state=42)
gdbc_model = GradientBoostingClassifier(random_state=42)
et_model = ExtraTreesClassifier(random_state=42)
svc_model = SVC(random_state=42)
xgb_model = xgb.XGBClassifier(seed=42,max_depth=3,learning_rate=0.1,n_estimators=40,random_state=42)
    #模型预测方法
def get_stacking(clf, x_train, y_train, x_test, n_folds=10):
    train_num, test_num = x_train.shape[0], x_test.shape[0]
    second_train = np.zeros((train_num,))
    second_test = np.zeros((test_num,))
    test_nfolds = np.zeros((test_num, n_folds))
    kf = KFold(n_splits=n_folds)
    for i, (train_index, test_index) in enumerate(kf.split(x_train)):
        x_tra, y_tra = x_train[train_index], y_train[train_index]
        x_tst, y_tst = x_train[test_index], y_train[test_index]
        clf.fit(x_tra, y_tra)
        second_train[test_index] = clf.predict(x_tst)
        test_nfolds[:, i] = clf.predict(x_test)
        second_test[:] = test_nfolds.mean(axis=1)
    return second_train, second_test
    #存放所有模型的结果
train_sets = []
test_sets = []
for clf in [rf_model, adb_model, gdbc_model, et_model, svc_model , xgb_model]:
    train_set, test_set = get_stacking(clf, X_train.values, y_train.values, X_test.values)
    train_sets.append(train_set)
    test_sets.append(test_set)
    #把所有训练集结果按列合并
meta_train = np.concatenate([result_set.reshape(-1,1) for result_set in train_sets], axis=1)
meta_test = np.concatenate([y_test_set.reshape(-1,1) for y_test_set in test_sets], axis=1)
    #二级分类器
from xgboost import XGBClassifier
xgb = XGBClassifier( colsample_bytree=0.8,
                    gamma=5,
                    learning_rate=0.05,
                    max_depth=3,
                    min_child_weight=2,
                    n_estimators=600,
                    random_state=42,
                    reg_alpha=2,
                    reg_lambda=2,
                    subsample=0.75)
xgb.fit(meta_train, y_train)
y_score = xgb.predict(meta_train)
y_score_test = xgb.predict(meta_test)

# #搜索超参数
# import xgboost as xgb
# from sklearn.model_selection import StratifiedKFold
# from sklearn.model_selection import GridSearchCV
# xgb_model = xgb.XGBClassifier(seed=42)
# max_depth = [3]
# learning_rate = [0.1]
# n_estimators = [40]
# param_grid = dict(max_depth = max_depth,learning_rate = learning_rate,n_estimators =n_estimators)
# kflod = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
#
# # 训练模型
# grid_search = GridSearchCV(xgb_model,param_grid,cv = kflod)
# grid_search = grid_search.fit(X_train, y_train)
# xgb_model = grid_search.best_estimator_
# print(grid_search.best_params_)
#
# pred_test = xgb_model.predict(X_test)


#预测验证集上的得分
from sklearn import metrics
print(metrics.accuracy_score(y_test, y_score_test))
print(metrics.accuracy_score(y_train, y_score))

#对测试集也做相关操作
test_data = test_data.drop(['Cabin','Name','Ticket'],axis=1)
test_onehot = pd.get_dummies(test_data[['Sex','Embarked']])
test_data = pd.concat([test_data,test_onehot],axis=1)
PassengerId = test_data['PassengerId']
test_data = test_data.drop(['Sex','Embarked','Age','Embarked_Q','PassengerId'],axis=1)

    #模型融合，一级模型预测
test_data = test_data.fillna(0)
sets = []
for clf in [rf_model, adb_model, gdbc_model, et_model, svc_model , xgb_model]:
    set = clf.predict(test_data.values)
    sets.append(set)
data = np.concatenate([y_test_set.reshape(-1,1) for y_test_set in sets], axis=1)

#测试集预测保存
predictions = xgb.predict(data)
output = pd.DataFrame({'PassengerId': PassengerId, 'Survived': predictions})
output.to_csv('my_submission.csv', index=False)
print("Your submission was successfully saved!")


#基本就到这里了。优化先做特征，再搞模型 ，后续优化思路：模型融合的一级模型参数可以调整，二级分类器参数也可以搜索，深度挖掘特征（比如Name中的姓氏，title。单身和家庭情况。或者对丢弃字段做一个填充，比如年龄，进行模型填充，离散化）
```

其他参考：https://blog.csdn.net/Koala_Tree/article/details/78725881

​					https://www.kaggle.com/c/titanic/notebooks



# --------------##-----------



# 文本特征提取

根据计算  词的权重来找出一篇文档中的关键词。

关键词提取技术，包括有监督（打一部分标签，训练，在1中在手工找1，在训练），无监督（TF-IDF、TextRank、基于主题）



### TF-IDF

用于搜索、文档分类 、聚类。

TF-IDF的前提是词袋模型 ，比词袋模型减少了无用词的影响。

实现为CountVectorizer+TfidfTransformer   或者  TfidfVectorizer。



##### 词袋模型

词袋模型就是 忽略词序语法近义等，每个词相互独立，简单统计每篇文章token的次数。

词集模型就是 忽略词序语法近义等，每个词相互独立，简单统计每篇文章token是否出现。

单纯的词集，词袋模型一般要转换成onehot的形式使用。

词袋模型一般用于关键词权重计算。词集模型用的不多，猜测用于有监督模型中，代码见sklearn。

```python
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()

corpus = [
    'This is the first document.',
    'This is the second second document.',
    'And the third one.',
    'Is this the first document?',
]

X = vectorizer.fit_transform(corpus)
print(X.toarray())
print(vectorizer.get_feature_names())

print(vectorizer.transform(['Something completely new.']).toarray())

#ngram_range默认1个词为1个token，可以设置1，2，3个词都是1个token，这样可以把词的顺序信息训练进来
bigram_vectorizer = CountVectorizer(ngram_range=(1, 3))
X_2 = bigram_vectorizer.fit_transform(corpus).toarray()

print(X_2)
print(bigram_vectorizer.get_feature_names())
```



##### TfidfTransformer

TF 词频

![20180806135836378](picture/20180806135836378.png)



IDF 逆文档频率 ：即词频的权重

![20180806140023860](picture/20180806140023860.png)

TF-IDF = TF*IDF

```python
from sklearn.feature_extraction.text import TfidfTransformer
transformer = TfidfTransformer(smooth_idf=False)

#一个列表代表一篇文章，共有6篇文章
#每一列代表一个词，3代表第一个词在第一篇文章里出现了3次。
#每一篇文章的每一个词都可以计算出tf-idf，最后把结果归一化
counts = [[3, 0, 1],
          [2, 0, 0],
          [3, 0, 0],
          [4, 0, 0],
          [3, 2, 0],
          [3, 0, 2]]

tfidf = transformer.fit_transform(counts)
print(tfidf.toarray())
```



##### TfidfVectorizer

CountVectorizer  +  TfidfTransformer

```python
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
vectorizer.fit_transform(corpus)
```



### TextRank

##### PageRank

TextRank 的思想来源于 PageRank。

PageRank：https://www.cnblogs.com/jpcflyer/p/11180263.html

```python
#PageRank 过程
给每个点（网页）初始权重。
写出转移矩阵（转移矩阵的理论基础就是那个pr计算公式，按改良公式进行转移得保证总量不变）。
根据转移矩阵更新初始权重。直至权重不再变化。
不再变化的权重就是那个网页的pr值（pr值不再变化的时候就完全反应了累积过后的【转移关系(用户意愿)】）。
```



##### TextRank

https://www.cnblogs.com/motohq/p/11887420.html

TextRank 与 PageRank相比，不同是每个词语之间都是双向边，且边有权重（双向边代表在同一句子中出现，边的权重代表出现次数或者相似度）。表现为 转移矩阵或者说理论公式不同。

 

本质是用词之间的关系找出关键词。所以 这种计算方法一定要过滤掉停用词。

```python
#TextRank 过程
1.把文本 按 句子进行分割
2.对每个句子进行 分词和词性标注处理，并过滤掉停用词，只保留指定词性的单词。
3.按PageRank 过程 构建图，计算出每个词的权重。
4.选topN 作为 候选关键词。
5.看候选关键词是否在文本中可以组成关键词组，把关键词组也作为关键词。
```

```python
import pandas as pd
import numpy as np
from textrank4zh import TextRank4Keyword, TextRank4Sentence

# 关键词抽取

def keywords_extraction(text):
    tr4w = TextRank4Keyword(allow_speech_tags=['n', 'nr', 'nrfg', 'ns', 'nt', 'nz'])
    # allow_speech_tags   --词性列表，用于过滤某些词性的词
    tr4w.analyze(text=text, window=2, lower=True, vertex_source='all_filters', edge_source='no_stop_words',
                 pagerank_config={'alpha': 0.85, })
    # text    --  文本内容，字符串
    # window  --  窗口大小，int，用来构造单词之间的边。默认值为2
    # lower   --  是否将英文文本转换为小写，默认值为False
    # vertex_source  -- 选择使用words_no_filter, words_no_stop_words, words_all_filters中的哪一个来构造pagerank对应的图中的节点
    #                -- 默认值为`'all_filters'`，可选值为`'no_filter', 'no_stop_words', 'all_filters'
    # edge_source  -- 选择使用words_no_filter, words_no_stop_words, words_all_filters中的哪一个来构造pagerank对应的图中的节点之间的边
    #              -- 默认值为`'no_stop_words'`，可选值为`'no_filter', 'no_stop_words', 'all_filters'`。边的构造要结合`window`参数

    # pagerank_config  -- pagerank算法参数配置，阻尼系数为0.85
    keywords = tr4w.get_keywords(num=6, word_min_len=2)
    # num           --  返回关键词数量
    # word_min_len  --  词的最小长度，默认值为1
    return keywords

# 关键短语抽取
def keyphrases_extraction(text):
    tr4w = TextRank4Keyword()
    tr4w.analyze(text=text, window=2, lower=True, vertex_source='all_filters', edge_source='no_stop_words',
                 pagerank_config={'alpha': 0.85, })
    keyphrases = tr4w.get_keyphrases(keywords_num=6, min_occur_num=1)
    # keywords_num    --  抽取的关键词数量
    # min_occur_num   --  关键短语在文中的最少出现次数
    return keyphrases

# 关键句抽取
def keysentences_extraction(text):
    tr4s = TextRank4Sentence()
    tr4s.analyze(text, lower=True, source='all_filters')
    # text    -- 文本内容，字符串
    # lower   -- 是否将英文文本转换为小写，默认值为False
    # source  -- 选择使用words_no_filter, words_no_stop_words, words_all_filters中的哪一个来生成句子之间的相似度。
    # 		  -- 默认值为`'all_filters'`，可选值为`'no_filter', 'no_stop_words', 'all_filters'
    # sim_func -- 指定计算句子相似度的函数

    # 获取最重要的num个长度大于等于sentence_min_len的句子用来生成摘要
    keysentences = tr4s.get_key_sentences(num=3, sentence_min_len=6)
    return keysentences

if __name__ == "__main__":
    text = "来源：中国科学报本报讯（记者肖洁）又有一位中国科学家喜获小行星命名殊荣！4月19日下午，中国科学院国家天文台在京举行“周又元星”颁授仪式，" \
           "我国天文学家、中国科学院院士周又元的弟子与后辈在欢声笑语中济济一堂。国家天文台党委书记、" \
           "副台长赵刚在致辞一开始更是送上白居易的诗句：“令公桃李满天下，何须堂前更种花。”" \
           "据介绍，这颗小行星由国家天文台施密特CCD小行星项目组于1997年9月26日发现于兴隆观测站，" \
           "获得国际永久编号第120730号。2018年9月25日，经国家天文台申报，" \
           "国际天文学联合会小天体联合会小天体命名委员会批准，国际天文学联合会《小行星通报》通知国际社会，" \
           "正式将该小行星命名为“周又元星”。"
    # 关键词抽取
    keywords =keywords_extraction(text)
    print(keywords)

    # 关键短语抽取
    keyphrases =keyphrases_extraction(text)
    print(keyphrases)

    # 关键句抽取
    keysentences =keysentences_extraction(text)
    print(keysentences)
```



### 主题模型

主题模型是生成模型。生成模型与判别模型的区别在于：判别模型直接计算出概率，生成模型比较各个可能的概率，选最大的



##### LSA

https://zhuanlan.zhihu.com/p/46376672

```python
#LSA过程
1.首先计算出 TF-IDF
2.进行svd矩阵分解 特征向量
3.利用余弦相似性找出相似文本
```

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#读数据
from sklearn.datasets import fetch_20newsgroups
dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))
documents = dataset.data
dataset.target_names

#删除符号与短词
news_df = pd.DataFrame({'document':documents})
news_df['clean_doc'] = news_df['document'].str.replace("[^a-zA-Z#]", " ")
news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))
news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())

#删除停用词
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())
tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])
detokenized_doc = []
for i in range(len(news_df)):
    t = ' '.join(tokenized_doc[i])
    detokenized_doc.append(t)
news_df['clean_doc'] = detokenized_doc

#TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words='english',max_df = 0.5, max_features= 1000,smooth_idf=True)
X = vectorizer.fit_transform(news_df['clean_doc'])

# SVD
from sklearn.decomposition import TruncatedSVD
svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)
svd_model.fit(X)

#主题排序
terms = vectorizer.get_feature_names()
for i, comp in enumerate(svd_model.components_):
    terms_comp = zip(terms, comp)
    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]
    print("Topic "+str(i)+": ",end='')
    for t in sorted_terms:
        print(t[0],end=' ')
    print(" ")
```

pLSA

LDA

HDP

LDA2Vec

# 赋权法

赋权法用于需要 确定权重系数的情况。分为主观赋权法和客观赋权法。



## 客观赋权法

### 熵权法

熵权法 计算  **信息熵**  来确定各个变量  **权重**  ，之后通过**变量和权重**可以计算出该条样本的  **评分**，用于挑选样本。

推导过程：https://www.zhihu.com/question/357680646/answer/943628631

```python
import pandas as pd
import numpy as np
import math
from numpy import array

df = pd.read_csv('aaa.csv')

#该方法要求x全为数值型
def cal_weight(x):
    x = x.apply(lambda x: ((x - np.min(x)) / (np.max(x) - np.min(x))))
    # 求k
    rows = x.index.size  # 行
    cols = x.columns.size  # 列

    k = 1.0 / math.log(rows)

    lnf = [[None] * cols for i in range(rows)]
    # 矩阵计算--
    # 信息熵
    # p=array(p)
    x = array(x)
    lnf = [[None] * cols for i in range(rows)]
    lnf = array(lnf)

    for i in range(0, rows):
        for j in range(0, cols):
            if x[i][j] == 0:
                lnfij = 0.0
            else:
                p = x[i][j] / x.sum(axis=0)[j]
                lnfij = math.log(p) * p * (-k)
            lnf[i][j] = lnfij
    lnf = pd.DataFrame(lnf)
    E = lnf

    # 计算冗余度
    d = 1 - E.sum(axis=0)
    # 计算各指标的权重
    w = [[None] * 1 for i in range(cols)]
    for j in range(0, cols):
        wj = d[j] / sum(d)
        w[j] = wj
        # 计算各样本的综合得分,用最原始的数据

    w = pd.DataFrame(w)
    return w

w = cal_weight(df)
w.index = df.columns
w.columns = ['weight']

df['score'] = 0
for i in df.columns:
    if i != 'score':
        df['score'] = df['score'] + df[i]*w.loc[i,'weight']

# w 是每个特征的权重，df['score'] 是每条数据的得分
```



## 主观赋权法

### 层次分析法

层次分析法本质上通过人的主观判断给变量不同的权重，然后再通过验证，最终确定权重。确定权重之后即可给每条数据打分。

使用流程：https://blog.csdn.net/lengxiao1993/article/details/19575261

```python
import numpy as np
import pandas as pd

# # 需要人手动给出比较矩阵
# compare_matrix = np.array([[1, 0.2, 0.33, 1],
#                           [5, 1, 1.66, 5],
#                           [3, 0.6, 1, 3],
#                           [1, 0.2, 0.33, 1]])

# # 一致性检验
# def isConsist(F):
#     n = np.shape(F)[0]
#     a, b = np.linalg.eig(F)
#     maxlam = a[0].real
#     CI = (maxlam - n) / (n - 1)
#     if CI < 0.1:
#         return bool(1)
#     else:
#         return bool(0)
# # 一致性检验异常
# class isConsistError(Exception):
#     def __init__(self, value):
#         self.value = value

#     def __str__(self):
#         return repr(self.value)

# if isConsist(compare_matrix) == False:
#     raise isConsistError('一致性检验不通过')

# # 根据相对矩阵，算出每一个影响因素的重要程度
# def ReImpo(F):
#     n = np.shape(F)[0]
#     W = np.zeros([1, n])
#     for i in range(n):
#         t = 1
#         for j in range(n):
#             t = F[i, j] * t
#         W[0, i] = t ** (1 / n)
#     W = W / sum(W[0, :])  # 归一化 W=[0.874,2.467,0.464]
#     return W.T
# W = ReImpo(compare_matrix)


# #此矩阵的每一行代表一个方案，每一列代表一个影响因素/此矩阵为方案层权重矩阵，大部分场景可以自动生成
# df = pd.read_csv('entropy_weight.csv')

# #归一化方案层权重矩阵
# for i in df:
#     df[i] = df[i]/sum(df[i])
    
    
# score = np.dot(df,W)
# score = pd.DataFrame(score)
# print(score)
```





# 待整理



[embedding32.docx](picture/embedding32.docx)



各种分布学习，核函数学习

模型原理



## nlp

https://cloud.tencent.com/developer/article/1531043

https://github.com/LogicJake/competition_baselines/blob/master/competitions/dc_onecity/Baseline.ipynb



### 异常检测

##### 其他

```python
https://blog.csdn.net/qianfeng_dashuju/article/details/92795190
https://cloud.tencent.com/developer/news/397780
    
    
https://zhuanlan.zhihu.com/p/103996509?utm_source=wechat_session  
    




#基于模型的
https://juejin.im/post/5b6a44f55188251aa8294b8c
https://www.cnblogs.com/webRobot/p/11965155.html
#基于概率分布的模型（例如正态分布的标准差范围）、基于聚类的方法（例如KMeans，DBSCAN）、基于密度的方法（例如LOF）、基于分类的方法（例如KNN）、基于统计的方法（例如分位数法）
#格拉布斯法，单变量异常值检测
#基于距离，多变量异常值检测
```



##### 孤立森林

```python
from sklearn.ensemble import IsolationForest
import numpy as np

X = np.array([[-1, -1], [-2, -1], [-3, -2], [0, 0], [-20, 50], [3, 5]])

clf = IsolationForest(n_estimators=10, warm_start=True)
clf.fit(X)

clf.set_params(n_estimators=20)
clf.fit(X) 
```

## 召回算法

召回算法属于推荐系统



### 关联规则

常用于找出用户经常同时购买的商品 用于推荐。



名词解释：

项集：若干个项的集合，项多数指代某件商品。2个项构成二项集，3个项构成三项集。

频繁项集：简单理解为频繁出现的项集。如果某项集的支持度大于最小支持度，为频繁项集。

支持度：所有数据中包括该项集样本所占的比例，为该项集的支持度

关联规则：在某个频繁项集的条件下推出另一个频繁项集。置信度大于最小置信度的关联规则为强关联规则

置信度：某个关联规则的概率，计算为P(B|A)。



##### Apriori

实际推荐系统中的关联规则多用FP-growth，Apriori因为要多次扫描数据库，内存开销过大，实际很难落地。

推导过程：https://www.jianshu.com/p/26d61b83492e

数据：[store_data.csv](picture/store_data.csv)

```python
#导包
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from apyori import apriori

#读入数据
data = pd.read_csv('store_data.csv')

#构成项集
records = []
for i in range(0, data.shape[0]):
    records.append([str(data.values[i,j]) for j in range(0, data.shape[1]) if str(data.values[i,j] != 'nan')])

#关联规则
# min_lift  最小支持度 ，min_length 频繁项集的最小数量。
association_rules = apriori(records, min_support=0.0045, min_confidence=0.2, min_lift=3, min_length=2)
association_results = list(association_rules)

#输出整理
Rule = []
Support = []
Confidence = []
Lift = []
for item in association_results:
    pair = item[0]
    items = [x for x in pair]

    print("Rule: " + items[0] + " -> " + items[1])
    Rule.append(items[0] + " -> " + items[1])

    print("Support: " + str(item[1]))
    Support.append(str(item[1]))

    print("Confidence: " + str(item[2][0][2]))
    Confidence.append(str(item[2][0][2]))

    print("Lift: " + str(item[2][0][3]))
    Lift.append(str(item[2][0][3]))

    print("=====================================")

#输出
data = pd.DataFrame()
data['Rule'] = Rule
data['Support'] = Support
data['Confidence'] = Confidence
data['Lift'] = Lift

data = data.sort_values(by='Confidence')
```



### Embedding



模型  Word2Vec

gensim，jieba基本实现了无监督的方法，这先不用

## 在线学习

[在线学习.md](picture/在线学习.md)



### xlearn

官网：https://xlearn-doc.readthedocs.io/en/latest/index.html



##### fm

```python
import xlearn as xl
fm_model=xl.FMModel(task="binary",lr=0.2,reg_lambda=0.002,metric="prec")
fm_model.fit(X_train,y_train)
y_pred = fm_model.predict(X_test)
print('AUC: %.4f' % metrics.roc_auc_score(y_test, y_pred))
```



##### ftrl

```python
lr_ftrl_model=xl.LRModel(task='binary', init=0.1,epoch=10, lr=0.1,reg_lambda=1.0, opt='ftrl')

lr_ftrl_model.fit(X_train,y_train)
y_pred = lr_ftrl_model.predict(X_test)
```

