# 数据划分

划分训练集和验证集，训练集用于模型训练，验证集用于模型评价

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
```



# 监督学习

### SVM

```python
#回归
from sklearn import svm
model_SVR = svm.SVR()
```



### KNN

```python
#回归
from sklearn import neighbors
model_KNeighborsRegressor = neighbors.KNeighborsRegressor()
```



### 决策树

树模型要求样本数要要远大于特征数，否则很容易过拟合。

```python
#二分类，多分类，排序
from sklearn import tree
X = [[0, 0], [1, 1]]
Y = [0, 1]
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)
clf.predict([[2., 2.]])
clf.predict_proba([[2., 2.]])
```

```python
#回归
from sklearn import tree
X = [[0, 0], [2, 2]]
y = [0.5, 2.5]
clf = tree.DecisionTreeRegressor()
clf = clf.fit(X, y)
clf.predict([[1, 1]])
```



### 随机森林

随机森林抽取数据采取的是自助采样法（即有放回的抽取数据）

随机森林分类最后结果的确定是通过软投票（每个分类器出所有类的概率，计算每个类的概率的加权平均值，取最大的类），而不是硬投票（每个分类器一个标签，最后结果是最多的）

```python
#分类
from sklearn.ensemble import RandomForestClassifier
X = [[0, 0], [1, 1]]
Y = [0, 1]
clf = RandomForestClassifier(n_estimators=10)
clf = clf.fit(X, Y)
clf.feature_importances_

#回归
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=20)
```



### AdaBoost

```python
#二分类
from sklearn.datasets import load_iris
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import AdaBoostClassifier

iris = load_iris()
clf = AdaBoostClassifier(n_estimators=100)
scores = cross_val_score(clf, iris.data, iris.target)

#回归
from sklearn.ensemble import AdaBoostRegressor
model = AdaBoostRegressor(n_estimators=50)
```



### GBDT

```python
#分类
from sklearn.ensemble import GradientBoostingClassifier
clf = GradientBoostingClassifier(n_estimators=100,learning_rate=1.0).fit(X_train, y_train)
clf.score(X_test, y_test)  

#回归
from sklearn.ensemble import GradientBoostingRegressor
est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1,loss='ls').fit(X_train, y_train)
```

| 参数                     | 默认值         | 解释                        |
| ------------------------ | -------------- | --------------------------- |
| n_estimators             | 100            | 基分类器个数，一般不超过500 |
| max_depth                | 3              | 最大树深度，2-3 即可        |
| max_features             | 400            | 最大特征数，一般不超过400   |
| subsample                | 1.0            |                             |
| random_state             | 42             |                             |
| learning_rate            | 0.1            |                             |
| warm_start               | False          |                             |
| tol                      | 1e-4           |                             |
| min_samples_split        | 2              |                             |
| min_samples_leaf         | 1              |                             |
| min_weight_fraction_leaf | 0              |                             |
| min_impurity_decrease    | 0              |                             |
| max_leaf_nodes           | None           |                             |
| n_iter_no_change         | None           |                             |
| min_impurity_split       | None           |                             |
| validation_fraction      | 0.1            |                             |
| ccp_alpha                | 0              |                             |
| loss                     | 'deviance'     |                             |
| criterion                | 'friedman_mse' |                             |
| presort                  | 'deprecated'   |                             |
| init                     | None           |                             |
| verbose                  | 0              | 日志冗长度，默认不输出      |



### 逻辑回归 

```python
#二分类，多分类，排序
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
X, y = load_iris(return_X_y=True)
clf = LogisticRegression(random_state=0).fit(X, y)
clf.predict(X[:2, :])
clf.predict_proba(X[:2, :])
clf.score(X, y)
```

| 参数              | 默认值 | 解释                                                         |
| ----------------- | ------ | ------------------------------------------------------------ |
| max_iter          |        | 最大迭代次数                                                 |
| tol               |        | 训练时停止求解的标准                                         |
| solver            | lbfgs  | 优化损失函数算法。                      <br />小数据：‘liblinear’                         <br />中数据：‘newton-cg’，‘lbfgs’       <br />大数据：‘sag’，‘saga’ |
| penalty           | l2     | 正则化。 与损失函数优化有关                     <br />‘liblinear’：                             ‘l1’，‘l2’          <br />‘newton-cg’，‘lbfgs’，‘sag’：‘l2’，‘none’    <br />‘saga’：                                    ‘elasticnet’，‘l1’，‘l2’，‘none’ |
| l1_ratio          |        | 正则化为'elasticnet' 有效，取值0-1。0代表完全l2，1代表完全l1 |
| C                 |        | 正则化系数λ的倒数，越小的数值表示越强的正则化                |
| random_state      | None   |                                                              |
| n_jobs            | None   | 并行数，-1代表所有                                           |
| warm_start        | False  | 热启动，使用上一次训练结果继续训练                           |
| verbose           |        | 日志冗长度，默认不输出                                       |
| multi_class       | auto   | 多分类的方式：‘auto’, ‘ovr’, ‘multinomial’                   |
| fit_intercept     |        | 目标函数是否有截距                                           |
| intercept_scaling | 1      | 防止正则化对截距系数的影响                                   |

| 罚项                             | `liblinear` | `lbfgs` | `newton-cg` | `sag` | `saga` |
| -------------------------------- | ----------- | ------- | ----------- | ----- | ------ |
| 多项式损失+L2罚项                | ×           | √       | √           | √     | √      |
| 一对剩余（One vs Rest） + L2罚项 | √           | √       | √           | √     | √      |
| 多项式损失 + L1罚项              | ×           | ×       | ×           | ×     | √      |
| 一对剩余（One vs Rest） + L1罚项 | √           | ×       | ×           | ×     | √      |
| 弹性网络                         | ×           | ×       | ×           | ×     | √      |
| 无罚项                           | ×           | √       | √           | √     | √      |
| **表现**                         |             |         |             |       |        |
| 惩罚偏置值(差)                   | √           | ×       | ×           | ×     | ×      |
| 大数据集上速度快                 | ×           | ×       | ×           | √     | √      |
| 未缩放数据集上鲁棒               | √           | √       | √           | ×     | ×      |



### 线性回归

```python
#回归
from sklearn import linear_model
reg = linear_model.LinearRegression()
reg.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
print(reg.coef_)
```



### XGBooost

官网：https://xgboost.readthedocs.io/en/latest/



sklearn 写法

```python
#二分类、多分类、排序、回归
import xgboost
clf = xgboost.XGBClassifier()
clf.fit(X_train,y_train)
y_prob = clf.predict_proba(X_test)[:,1]                          
y_pred = clf.predict(X_test)                                        

#获取特征重要性 图表
from xgboost import plot_importance
plot_importance(clf,max_num_features=5)

df_importance = pd.DataFrame()          
df_importance['column_name'] = train_x.columns
df_importance['importance'] = model.feature_importances_
df_importance.sort_values(by='importance')
```

sklearn--XGBClassifier 参数

```python
#booster 基分类器
    gbtree 树模型做为基分类器（默认）
    gbliner 线性模型做为基分类器
#nthread=1  所用cpu核数
    nthread=-1时，使用全部CPU进行并行运算（默认）
#scale_pos_weight	正样本的权重，
	在二分类任务中，当正负样本比例失衡时。scale_pos_weight=负样本数量/正样本数量。

#n_estimatores=100
	基模型个数 
#early_stopping_rounds=5
	当迭代5次后，仍没有提高，提前停止训练
#max_depth=6
	树的深度，默认值为6，典型值3-10。
#learning_rate=0.3
	学习率，控制每次迭代更新权重时的步长，默认0.3。调参：值越小，训练越慢。典型值为0.01-0.2。
#alpha
    L1正则化系数，默认为1
#lambda
    L2正则化系数，默认为1
 
#min_child_weight=1
	值越小，越容易过拟合
#subsample=1  抽样本比例
	训练每棵树时，使用的数据占全部训练集的比例。默认值为1，典型值为0.5-1。防止overfitting。
#colsample_bytree  抽特征比例
	训练每棵树时，使用的特征占全部特征的比例。默认值为1，典型值为0.5-1。防止overfitting。
#gamma	惩罚项系数
	指定节点分裂所需的最小损失函数下降值。默认0

#objective 目标函数
    回归任务
        reg:linear (默认)
        reg:logistic 
    二分类
        binary:logistic     概率 
        binary：logitraw   类别
    多分类
        multi：softmax  num_class=n   返回类别
        multi：softprob   num_class=n  返回概率
    rank:pairwise 
#eval_metric
    回归任务(默认rmse)
        rmse--均方根误差
        mae--平均绝对误差
    分类任务(默认error)
        auc--roc曲线下面积
        error--错误率（二分类）
        merror--错误率（多分类）
        logloss--负对数似然函数（二分类）
        mlogloss--负对数似然函数（多分类）
```



### 多标签/多输出

##### 概述

Binary classification（二分类/回归）

Multiclass classification（多分类）

Multilabel classification（多标签分类/多输出二分类）

Multioutput regression （多输出回归）

Multioutput-multiclass classification（多输出多分类）/ multi-task classification（多任务分类）



*tips：*

**多分类** 和 **多标签分类** 和 **多输出回归** 原理：通过 multiclass 把这些问题转化为二分类问题



##### 二分类、多分类

二分类/回归、多分类： sklearn 的所有分类器对这两个类 开箱即用

```
固有多分类:
        sklearn.naive_bayes.BernoulliNB
        sklearn.tree.DecisionTreeClassifier
        sklearn.tree.ExtraTreeClassifier
        sklearn.ensemble.ExtraTreesClassifier
        sklearn.naive_bayes.GaussianNB
        sklearn.neighbors.KNeighborsClassifier
        sklearn.semi_supervised.LabelPropagation
        sklearn.semi_supervised.LabelSpreading
        sklearn.discriminant_analysis.LinearDiscriminantAnalysis
        sklearn.svm.LinearSVC (setting multi_class=”crammer_singer”)
        sklearn.linear_model.LogisticRegression (setting multi_class=”multinomial”)
        sklearn.linear_model.LogisticRegressionCV (setting multi_class=”multinomial”)
        sklearn.neural_network.MLPClassifier
        sklearn.neighbors.NearestCentroid
        sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis
        sklearn.neighbors.RadiusNeighborsClassifier
        sklearn.ensemble.RandomForestClassifier
        sklearn.linear_model.RidgeClassifier
        sklearn.linear_model.RidgeClassifierCV
1对1-多分类:
        sklearn.svm.NuSVC
        sklearn.svm.SVC.
        sklearn.gaussian_process.GaussianProcessClassifier (setting multi_class = “one_vs_one”)
1对多-多分类:
        sklearn.ensemble.GradientBoostingClassifier
        sklearn.gaussian_process.GaussianProcessClassifier (setting multi_class = “one_vs_rest”)
        sklearn.svm.LinearSVC (setting multi_class=”ovr”)
        sklearn.linear_model.LogisticRegression (setting multi_class=”ovr”)
        sklearn.linear_model.LogisticRegressionCV (setting multi_class=”ovr”)
        sklearn.linear_model.SGDClassifier
        sklearn.linear_model.Perceptron
        sklearn.linear_model.PassiveAggressiveClassifier
```



##### 多标签分类

y有多个标签，如新闻主题可以为 政治、金融、 教育 中的一个或几个。

```python
#将多标签分类转化为多输出二分类的格式
from sklearn.preprocessing import MultiLabelBinarizer
y = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]]

MultiLabelBinarizer().fit_transform(y)

>>> array([[0, 0, 1, 1, 1],
       [0, 0, 1, 0, 0],
       [1, 1, 0, 1, 0],
       [1, 1, 1, 1, 1],
       [1, 1, 1, 0, 0]])
```

```python
#多标签分类/多输出二分类
from sklearn.datasets import make_multilabel_classification
from sklearn.model_selection import train_test_split
X, Y = make_multilabel_classification(n_samples=10, n_features=5, n_classes=3, n_labels=2)
X_train, X_test, Y_train ,Y_test = train_test_split(X, Y, test_size=0.2)

from sklearn.tree import DecisionTreeClassifier
cls = DecisionTreeClassifier()
cls.fit(X_train, Y_train)

Y_pred = cls.predict(X_test)

from sklearn import metrics
metrics.f1_score(Y_test, Y_pred, average="macro")
```

```
多标签分类/固有的多输出二分类:
        sklearn.tree.DecisionTreeClassifier
        sklearn.tree.ExtraTreeClassifier
        sklearn.ensemble.ExtraTreesClassifier
        sklearn.neighbors.KNeighborsClassifier
        sklearn.neural_network.MLPClassifier
        sklearn.neighbors.RadiusNeighborsClassifier
        sklearn.ensemble.RandomForestClassifier
        sklearn.linear_model.RidgeClassifierCV
```



##### 多输出回归

1.固有多输出回归

```
sklearn.linear_model.LinearRegression
sklearn.neighbors.KNeighborsRegressor
sklearn.tree.DecisionTreeRegressor
sklearn.ensemble.RandomForestRegressor
```

```python
from sklearn.datasets import make_regression
from sklearn.linear_model import LinearRegression

X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, n_targets=2, random_state=1)
data_in = [[-2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381, 0.87616892, -0.50446586, 0.23009474, 0.76201118]]

model = LinearRegression()
model.fit(X, y)

yhat = model.predict(data_in)
```



2.每一输出单独建立一个回归模型

MultiOutputRegressor

```python
#假设每个输出之间都是相互独立的
from sklearn.datasets import make_regression
from sklearn.multioutput import MultiOutputRegressor
from sklearn.ensemble import GradientBoostingRegressor

X, y = make_regression(n_samples=10, n_targets=3, random_state=1)

MultiOutputRegressor(GradientBoostingRegressor(random_state=0)).fit(X, y).predict(X)

>>> array([[-154.75474165, -147.03498585,  -50.03812219],
       [   7.12165031,    5.12914884,  -81.46081961],
       [-187.8948621 , -100.44373091,   13.88978285],
       [-141.62745778,   95.02891072, -191.48204257],
       [  97.03260883,  165.34867495,  139.52003279],
       [ 123.92529176,   21.25719016,   -7.84253   ],
       [-122.25193977,  -85.16443186, -107.12274212],
       [ -30.170388  ,  -94.80956739,   12.16979946],
       [ 140.72667194,  176.50941682,  -17.50447799],
       [ 149.37967282,  -81.15699552,   -5.72850319]])
```



3.每个输出都参与的链接模型

RegressorChain

```python
#第一个模型的输入和输出作为第二个模型的输入，以此类推
from sklearn.datasets import make_regression
from sklearn.multioutput import RegressorChain
from sklearn.svm import LinearSVR

X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, n_targets=2, random_state=1)
data_in = [[-2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381, 0.87616892, -0.50446586, 0.23009474, 0.76201118]]

model = LinearSVR()
wrapper = RegressorChain(model)

wrapper.fit(X, y)

yhat = wrapper.predict(data_in)
```



##### 多输出多分类

目前,sklearn.metrics中没有评估方法能够支持多输出多分类任务。



1.固有的多输出多分类

```
sklearn.tree.DecisionTreeClassifier
sklearn.tree.ExtraTreeClassifier
sklearn.ensemble.ExtraTreesClassifier
sklearn.neighbors.KNeighborsClassifier
sklearn.neighbors.RadiusNeighborsClassifier
sklearn.ensemble.RandomForestClassifier
```



2.每一个输出单独建立一个分类模型

```python
#假设每个输出之间都是相互独立的
from sklearn.datasets import make_classification
from sklearn.multioutput import MultiOutputClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils import shuffle
import numpy as np

X, y1 = make_classification(n_samples=10, n_features=100, n_informative=30, n_classes=3, random_state=1)
y2 = shuffle(y1, random_state=1)
y3 = shuffle(y1, random_state=2)
Y = np.vstack((y1, y2, y3)).T

forest = RandomForestClassifier(n_estimators=100, random_state=1)

multi_target_forest = MultiOutputClassifier(forest, n_jobs=-1)

multi_target_forest.fit(X, Y).predict(X)

>>> array([[2, 2, 0],
       [1, 2, 1],
       [2, 1, 0],
       [0, 0, 2],
       [0, 2, 1],
       [0, 0, 2],
       [1, 1, 0],
       [1, 1, 1],
       [0, 0, 2],
       [2, 0, 0]])
```

3.ClassifierChain

见多输出回归对应部分或者官网。



# 无监督学习

### 聚类模型

聚类一定要选好特征，才能使结果具有可解释型。

| Method name（方法名称）                                      | Parameters（参数）                                           | Scalability（可扩展性）                                      | Usecase（使用场景）                                          | Geometry (metric used)（几何图形（公制使用））               |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| [K-Means](https://sklearn.apachecn.org/docs/master/22.html#k-means) | number of clusters（聚类形成的簇的个数）                     | 非常大的 `n_samples`, 中等的 `n_clusters` 使用 [MiniBatch 代码）](https://sklearn.apachecn.org/docs/master/22.html#mini-batch-kmeans) | 通用, 均匀的 cluster size（簇大小）, flat geometry（平面几何）, 不是太多的 clusters（簇） | Distances between points（点之间的距离）                     |
| [Affinity propagation](https://sklearn.apachecn.org/docs/master/22.html#affinity-propagation) | damping（阻尼）, sample preference（样本偏好）               | Not scalable with n_samples（n_samples 不可扩展）            | Many clusters, uneven cluster size, non-flat geometry（许多簇，不均匀的簇大小，非平面几何） | Graph distance (e.g. nearest-neighbor graph)（图距离（例如，最近邻图）） |
| [Mean-shift](https://sklearn.apachecn.org/docs/master/22.html#mean-shift) | bandwidth（带宽）                                            | Not scalable with `n_samples` （`n_samples`不可扩展）        | Many clusters, uneven cluster size, non-flat geometry（许多簇，不均匀的簇大小，非平面几何） | Distances between points（点之间的距离）                     |
| [Spectral clustering](https://sklearn.apachecn.org/docs/master/22.html#spectral-clustering) | number of clusters（簇的个数）                               | 中等的 `n_samples`, 小的 `n_clusters`                        | Few clusters, even cluster size, non-flat geometry（几个簇，均匀的簇大小，非平面几何） | Graph distance (e.g. nearest-neighbor graph)（图距离（例如最近邻图）） |
| [Ward hierarchical clustering](https://sklearn.apachecn.org/docs/master/22.html#hierarchical-clustering) | number of clusters（簇的个数）                               | 大的 `n_samples` 和 `n_clusters`                             | Many clusters, possibly connectivity constraints（很多的簇，可能连接限制） | Distances between points（点之间的距离）                     |
| [Agglomerative clustering](https://sklearn.apachecn.org/docs/master/22.html#hierarchical-clustering) | number of clusters（簇的个数）, linkage type（链接类型）, distance（距离） | 大的 `n_samples` 和 `n_clusters`                             | Many clusters, possibly connectivity constraints, non Euclidean distances（很多簇，可能连接限制，非欧氏距离） | Any pairwise distance（任意成对距离）                        |
| [DBSCAN](https://sklearn.apachecn.org/docs/master/22.html#dbscan) | neighborhood size（neighborhood 的大小）                     | 非常大的 `n_samples`, 中等的 `n_clusters`                    | Non-flat geometry, uneven cluster sizes（非平面几何，不均匀的簇大小） | Distances between nearest points（最近点之间的距离）         |
| [Gaussian mixtures（高斯混合）](https://sklearn.apachecn.org/docs/master/mixture.html#mixture) | many（很多）                                                 | Not scalable（不可扩展）                                     | Flat geometry, good for density estimation（平面几何，适用于密度估计） | Mahalanobis distances to centers（ 与中心的马氏距离）        |
| [Birch](https://sklearn.apachecn.org/docs/master/22.html#birch) | branching factor（分支因子）, threshold（阈值）, optional global clusterer（可选全局簇）. | 大的 `n_clusters` 和 `n_samples`                             | Large dataset, outlier removal, data reduction.（大型数据集，异常值去除，数据简化） | Euclidean distance between points（点之间的欧氏距离）        |



##### 肘部观察法

使用肘部观察法选择最佳的聚类群数

```python
from yellowbrick.cluster.elbow import kelbow_visualizer
kelbow_visualizer(KMeans(random_state=42),df.drop(['institutes_id'],axis=1).drop(['institutes_name'],axis=1),k=(2,15))
```



##### K-means

```python
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans

X, y = make_blobs(n_samples=1500, random_state=42)
y_pred = KMeans(n_clusters=2, random_state=42).fit_predict(X)
```

```python
#查看聚类是否均匀
df.groupby('y_pred').count()

#查看聚类中心，根据每一个属性不同类的差异进行解释
pd.DataFrame(model.cluster_centers_ )

#查看每一类有多少种类的数据
df.loc[df['y_pred']==5,['institutes_type','big_area','year_price','avg_date']].drop_duplicates()
```



##### 层次聚类

```python
#Ward, complete,average,single linkage四种策略
from sklearn.cluster import AgglomerativeClustering
import numpy as np

X = np.array([[1, 2], [1, 4], [1, 0],[4, 2], [4, 4], [4, 0]])
clustering = AgglomerativeClustering(linkage='ward', n_clusters=2).fit(X)

clustering.labels_
```



##### DBSCAN

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

X1, y1=datasets.make_circles(n_samples=5000, factor=.6,noise=.05)
X2, y2 = datasets.make_blobs(n_samples=1000, n_features=2, centers=[[1.2,1.2]], cluster_std=[[.1]], random_state=9)

X = np.concatenate((X1, X2))
plt.scatter(X[:, 0], X[:, 1], marker='o')
plt.show()

from sklearn.cluster import DBSCAN
y_pred = DBSCAN(eps = 0.1, min_samples = 10).fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=y_pred)
plt.show()
```



### 降维模型

https://mp.weixin.qq.com/s/xl2jPL8hNdSUVe5PhqPdoA



##### PCA

主成分分析法

```python
#保证数据进行了无量纲化处理
from sklearn.decomposition import PCA

#参数n_components为主成分数目
PCA(n_components=2).fit_transform(iris.data)
```



##### LDA

线性判别分析法

```python
from sklearn.lda import LDA
LDA(n_components=2).fit_transform(iris.data, iris.target)
```



### 异常检测



# 半监督学习

`LabelPropagation`和`LabelSpreading`可以预测y缺失的数据集，只要把缺失部分换成-1即可预测

```python
#以下代码用于比较LabelSpreading预测y缺失数据集的准确度
#点的颜色是真实值。背景色是预测值
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn import svm
from sklearn.semi_supervised import LabelSpreading

iris = datasets.load_iris()
X = iris.data[:, :2]
y = iris.target
print(y)

#准备缺失y
rng = np.random.RandomState(0)
y_30 = np.copy(y)
y_30[rng.rand(len(y)) < 0.3] = -1
y_50 = np.copy(y)
y_50[rng.rand(len(y)) < 0.5] = -1

#准备模型
ls30 = (LabelSpreading().fit(X, y_30), y_30)
ls50 = (LabelSpreading().fit(X, y_50), y_50)
ls100 = (LabelSpreading().fit(X, y), y)
rbf_svc = (svm.SVC(kernel='rbf', gamma=.5).fit(X, y), y)

#准备画图
h = .02
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))
titles = ['Label Spreading 30% data',
          'Label Spreading 50% data',
          'Label Spreading 100% data',
          'SVC with rbf kernel']
color_map = {-1: (1, 1, 1), 0: (0, 0, .9), 1: (1, 0, 0), 2: (.8, .6, 0)}


for i, (clf, y_train) in enumerate((ls30, ls50, ls100, rbf_svc)):
    plt.subplot(2, 2, i + 1)

    #根据预测结果绘制背景
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)

    #每个点的x，y轴是特征，颜色是真实y
    colors = [color_map[y] for y in y_train]
    plt.scatter(X[:, 0], X[:, 1], c=colors, edgecolors='black')
    plt.title(titles[i])

plt.suptitle("Unlabeled points are colored white", y=0.1)
plt.show()
```



# 集成模型

**bagging**

并行出结果后投票（硬投票）或平均概率（软投票）

随机森林，bagging（相同模型bagging），voting（不同模型bagging）

可以减小过拟合，所以通常在强分类器和复杂模型上使用时表现的很好（例如，完全生长的决策树）



**boosting**

串行提升

GBDT、AdaBoost、xgbooost

在弱模型上表现更好（例如，浅层决策树）。

​																		

**stacking**



### bagging

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.neighbors import KNeighborsClassifier
bagging = BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5)

from sklearn.ensemble import BaggingRegressor
model_BaggingRegressor = BaggingRegressor()
```

抽取随机子集的随机特征 来构建多个估计器，能减小bagging元估计器的方差，更稳定

```python
#调参
max_samples 抽取样本比例
max_features 抽取特征比例
bootstrap 样本抽取是否有放回
bootstrap_features 特征抽取是否有放回
```



### Voting 

```python
#分类
#硬投票
from sklearn import datasets
iris = datasets.load_iris()
X, y = iris.data[:, 1:3], iris.target

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import cross_val_score

clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',random_state=1)
clf2 = RandomForestClassifier(n_estimators=50, random_state=1)
clf3 = GaussianNB()

eclf=VotingClassifier(estimators=[('lr', clf1),('rf', clf2),('gnb', clf3)],voting='hard')
                      
for clf, label in zip([clf1, clf2, clf3, eclf], 
					['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):
    scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')
    print("Accuracy: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(), label))
    
#软投票
eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)],
                        voting='soft', weights=[2, 1, 2])
```

```python
#回归
from sklearn import datasets
boston = datasets.load_boston()
X = boston.data
y = boston.target

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import VotingRegressor

reg1 = GradientBoostingRegressor(random_state=1, n_estimators=10)
reg2 = RandomForestRegressor(random_state=1, n_estimators=10)
reg3 = LinearRegression()
ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])
ereg = ereg.fit(X, y)
```



### stacking

把所有模型的预测结果变成一个样本，用xgb进行再预测

```python
import numpy as np
from sklearn.model_selection import KFold

#看不懂就自己写画矩阵
def get_stacking(clf, x_train, y_train, x_test, n_folds=10):
    """这个函数是stacking的核心，使用交叉验证的方法得到次级训练集
    x_train, y_train, x_test 的值应该为numpy里面的数组类型 numpy.ndarray .如果输入为pandas的DataFrame类型则会报错"""

    train_num, test_num = x_train.shape[0], x_test.shape[0]

    #这个矩阵用来存储所有验证集（即训练集）结果
    second_train = np.zeros((train_num,))
    #这个矩阵用来存储测试集（n个模型的预测均值）结果
    second_test = np.zeros((test_num,))
    # 这个矩阵用来存储测试集（所有模型的每次测试集）结果
    test_nfolds = np.zeros((test_num, n_folds))
    kf = KFold(n_splits=n_folds)

    #kf.split(x_train)返回一个包含n个元组的列表，每个元组是(train_index, test_index)其中train_index、test_index都是索引列表，enumerate会给n个元组加上索引
    for i, (train_index, test_index) in enumerate(kf.split(x_train)):

        #测试集
        x_tra, y_tra = x_train[train_index], y_train[train_index]
        #验证集
        x_tst, y_tst = x_train[test_index], y_train[test_index]

        clf.fit(x_tra, y_tra)
        #预测验证并填充second_train，循环之后所有的训练集数据都被验证过了
        second_train[test_index] = clf.predict(x_tst)
        #预测测试集x_test并用均值填充second_test
        test_nfolds[:, i] = clf.predict(x_test)
        second_test[:] = test_nfolds.mean(axis=1)
    #返回一个所有验证集的结果 和 所有模型测试集结果的均值
    return second_train, second_test

#导入模型
from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier,GradientBoostingClassifier, ExtraTreesClassifier)
from sklearn.svm import SVC
rf_model = RandomForestClassifier()
adb_model = AdaBoostClassifier()
gdbc_model = GradientBoostingClassifier()
et_model = ExtraTreesClassifier()
svc_model = SVC()

#造数据
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
iris = load_iris()
train_x, test_x, train_y, test_y = train_test_split(iris.data, iris.target, test_size=0.2)

#存放所有模型的结果
train_sets = []
test_sets = []
for clf in [rf_model, adb_model, gdbc_model, et_model, svc_model]:
    train_set, test_set = get_stacking(clf, train_x, train_y, test_x)
    train_sets.append(train_set)
    test_sets.append(test_set)

#把所有训练集结果按列合并
meta_train = np.concatenate([result_set.reshape(-1,1) for result_set in train_sets], axis=1)
meta_test = np.concatenate([y_test_set.reshape(-1,1) for y_test_set in test_sets], axis=1)

#使用决策树作为我们的次级分类器
from xgboost import XGBClassifier
xgb = XGBClassifier()

xgb.fit(meta_train, train_y)

y_score = xgb.predict(meta_train)
y_score_test = xgb.predict(meta_test)

from sklearn.metrics import accuracy_score
print(accuracy_score(train_y,y_score))
print(accuracy_score(test_y,y_score_test))

#上面是全量数据的融合
# 如果数据是训练集和预测集，且预测集没标签的形式，上面就是训练集（自己分训练集、验证集）的融合，下面也要对测试集进行同样的构造得到次级训练集
test_data = test_data.fillna(0)
sets = []
for clf in [rf_model, adb_model, gdbc_model, et_model, svc_model]:
    set = clf.predict(test_data.values)
    sets.append(set)
data = np.concatenate([y_test_set.reshape(-1,1) for y_test_set in sets], axis=1)
```



### GBDT+LR

```python
from sklearn import metrics
import pandas as pd
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.externals import joblib
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV

df=pd.read_csv('train_cleared.csv',sep=',')
df=df.fillna(0)

df_x=df.iloc[:, 1:]
df_y=df['label']

gbdt= GradientBoostingClassifier()
gbdt.fit(df_x,df_y)

leaves= gbdt.apply(df_x)[:,:,0]
X_train,X_test,y_train,y_test=train_test_split(leaves,df_y,random_state=7,test_size=0.33)

lr= LogisticRegression()
lr.fit(X_train,y_train)

y_pred=lr.predict(X_test)
print('AUC: %.4f' % metrics.roc_auc_score(y_test,y_pred))
```



### XGB+LR

```python
import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.datasets import load_svmlight_file
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.externals import joblib
from sklearn.preprocessing import  OneHotEncoder
import numpy as np
from scipy.sparse import hstack
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('test.csv', sep=',')
df = df.fillna(0)
y_all = df['label']
X_all = df.iloc[:, 1:]
X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.3, random_state = 42)

model = xgb.XGBClassifier(learning_rate=0.08,n_estimators=50,max_depth=5,gamma=0,subsample=0.9,    
                          colsample_bytree=0.5) 

model.fit(X_train.values, y_train.values)
y_pred = model.predict_proba(X_test.values)[0]

X_train_leaves = model.apply(X_train)
X_test_leaves = model.apply(X_test)
train_rows = X_train_leaves.shape[0]

# # 合并编码后的训练数据和测试数据
X_leaves = np.concatenate((X_train_leaves, X_test_leaves), axis=0)
X_leaves = X_leaves.astype(np.int32)
(rows, cols) = X_leaves.shape
xgbenc = OneHotEncoder()
X_trans = xgbenc.fit_transform(X_leaves)

df1 = pd.DataFrame(xgbenc.fit_transform(df_leaves).A).values.tolist() 

lr = LogisticRegression()
lr.fit(X_trans[:train_rows, :], y_train)
y_pred_xgblr1 = lr.predict_proba(X_trans[train_rows:, :])[:, 1]
xgb_lr_auc1 = roc_auc_score(y_test, y_pred_xgblr1)

# # 将数据分为训练集和测试集进行，用新的特征输入LR进行预测
lr = LogisticRegression(n_jobs=-1)
X_train_ext = hstack([X_trans[:train_rows, :], X_train])
X_test_ext = hstack([X_trans[train_rows:, :], X_test])
lr.fit(X_train_ext, y_train)
y_pred_xgblr2 = lr.predict_proba(X_test_ext)[:, 1]
xgb_lr_auc2 = roc_auc_score(y_test, y_pred_xgblr2)
print('基于组合特征的LR AUC: %.5f' % xgb_lr_auc2)
```



# 增量学习

sklearn 中支持增量学习的算法：

```
        Classification
                sklearn.naive_bayes.MultinomialNB
                sklearn.naive_bayes.BernoulliNB
                sklearn.linear_model.Perceptron
                sklearn.linear_model.SGDClassifier
                sklearn.linear_model.PassiveAggressiveClassifier
                sklearn.neural_network.MLPClassifier

        Regression
                sklearn.linear_model.SGDRegressor
                sklearn.linear_model.PassiveAggressiveRegressor
                sklearn.neural_network.MLPRegressor

        Clustering
                sklearn.cluster.MiniBatchKMeans
                sklearn.cluster.Birch

        Decomposition / feature Extraction
                sklearn.decomposition.MiniBatchDictionaryLearning
                sklearn.decomposition.IncrementalPCA
                sklearn.decomposition.LatentDirichletAllocation

        Preprocessing
                sklearn.preprocessing.StandardScaler
                sklearn.preprocessing.MinMaxScaler
                sklearn.preprocessing.MaxAbsScaler
```



# 时间序列

































