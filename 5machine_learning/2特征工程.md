思路参考：https://www.zhihu.com/question/29316149/answer/2346832545



# 模型衰减

特征选用均值特征，而不是累计值特征，更能抵抗模型衰减



# 数据采样

一般正负样本比大于1：10，即认为数据不均衡。不均衡数据需要进行采样处理，避免模型AUC过低。

https://www.zhihu.com/question/269698662/answer/352279936



### 过采样

随机过采样

```
从少数样本直接抽出数据作为新样本，可能会造成模型过拟合
```



smote算法

```python
#一般在过采样前要进行标准化，过采样后处理一下位数过多的结果
from imblearn.over_sampling import SMOTE
X_train,y_train = SMOTE(random_state=0).fit_sample(x, y)
```



基于k-means过采样

```
基于k-means聚类过采样方法一般分为两步：

    首先分别对正负例进行K-means聚类
    聚类之后，对其中较小的簇进行上面的过采样方法扩充样本数量
    然后在进行正负类样本均衡扩充

该算法不仅可以解决类间不平衡问题，而且还能解决类内部不平衡问题。
```



### 欠采样

随机欠采样

```
从多数类样本中随机抽取一部分数据进行删除，误差很大，一般不用
```



分层采样：根据时间提取：对每个月的数据进行欠采样。这里用的还是随机欠采样，也可以用其他欠采样方法。

```python
data_new = pd.DataFrame(columns=data.columns)
time = sorted(list(data.data_dt.unique()))
for i in range(len(time)):
    data_time = data[data.data_dt==time[i]]
    data_time_sample = data_time[data_time.y==0].sample(100000,axis=0)
    data_time = data_time[data_time.y==1].append(data_time_sample)
    print(time[i])
    print(data_time.shape)
    data_new = data_new.append(data_time)
```



基于k-means欠采样：步骤同过采样



### imbalanced-learn

专门用于采样的库

![v2-c2446593bf7c85c73f3715904ee24957_b](picture/v2-c2446593bf7c85c73f3715904ee24957_b-1606381549945.jpg)



# 单一值处理

```python
#画图看单一值
for i in df:
    if len(df[i].unique())==1:
        print("*******************特征",i,"为单一值，一般需要删除***********************")

#删除单一值的列
for i in df:
    if len(df[i].unique())==1:
        df = df.drop([i],axis=1)
```



# 重复值处理

大部分情况下不需要处理重复值

```python
#查看重复值
print(df.duplicated())

#删除重复值记录
df1 = df.drop_duplicates(['col1'])
```



# 查看缺失值

```python
#快速查看
df.isnull().sum()
```

```python
#查看缺失值
temp = df.isnull().sum().sort_values().reset_index()
print('共有'+str(df.shape[0])+'条数据')
for i in temp[0]:
    col = pd.DataFrame()
    if i!=0:
        col = temp.loc[temp[0]==i,'index'].iloc[0]
        print("*******************",col,"***********************")
        print('缺失数量：'+str(i))
        print('缺失比例：'+str(round((i/len(df)*100),2))+'%')
        print('字段类型'+str(df[col].dtypes))
del temp,col
```

```python
#如果某个pattern过多，考虑结构化缺失的可能性。
def row_miss(df):
    n_row, n_col = df.shape
    # 构建各行缺失pattern，e.g: '10001000110...'
    row_miss_check = df.applymap(lambda x: '1' if pd.isna(x) else '0')
    row_miss_pat = row_miss_check.apply(lambda x: ''.join(x.values), axis = 1)
    # 统计各pattern的频率
    pat_des = row_miss_pat.value_counts()
    print('The amount of missing patterns is {0}'.format(len(pat_des)))
    # 将各频率的累加表进行输出
    pat_des_df = pd.DataFrame(pat_des, columns=['row_cnt'])
    pat_des_df.index.name = 'miss_pattern'
    pat_des_df['cumsum_cover_pct'] = round(pat_des_df['row_cnt']/n_row, ndigits=2)
    
    if pat_des_df.shape[0]>10:
        print('The top 10 patterns info is:\n{0}'.format(pat_des_df.iloc[:10,:]))
    else:
        print('Patterns info is:\n{0}'.format(pat_des_df))
row_miss(df)

#结构化缺失，应该探究原因，同步处理同时缺失的列，不能针对单独的一列缺失处理。
```



# 缺失值处理

见sklearn笔记



### 不处理

缺失值较少的时候，不处理缺失值。



### 缺失值删除

```python
#删特征，缺失比例超过90%的特征，考虑直接删掉。
x_train = x_train.drop(['Cabin'],axis=1)
```

```python
#删数据，如果数据样本很多，但有很少的缺失值，可以删掉带缺失值的行
df = df.dropna(how='any')
df = df[~df['A'].isin([np.nan])]
```



### 缺失值填充

##### fillna

```python
#正态分布用均值
df = df.fillna(value=0)
df = df.fillna(value=df.mean())
df = df.fillna(value=df.median())
df = df.fillna(value=df.mode())
```

```python
#填充上下条数据
train_data.fillna(method='pad', inplace=True) # 填充前一条数据的值，但是前一条也不一定有值
train_data.fillna(method='bfill', inplace=True) # 填充后一条数据的值，但是后一条也不一定有值
train_data.fillna(0, inplace=True)
```

##### SimpleImputer

![1576119201911](picture/1576119201911-1603354042979.png)

```python
#一列数据要做处理，多列数据不用。
Age = data["Age"].values.reshape(-1,1)

from sklearn.impute import SimpleImputer
imp = SimpleImputer(strategy="constant",fill_value=0)
imp = imp.fit_transform(Age)

#输出是ndarray，所以赋值回去
data.loc[:,"Age"] = imp
```

##### 模型预测缺失值

适用于缺失变量与其他变量相关的情况

```python
#基于随机森林回归模型，把缺失值当成label，预测，进行填充
train = df.loc[~df['Age'].isnull(),:]
label = df.loc[~all_df['Age'].isnull(),'Age']	
 
test = df.loc[df['Age'].isnull(),:]

from sklearn.ensemble import RandomForestRegressor
rfr = RandomForestRegressor(random_state=0, n_estimators=2000)

rfr.fit(train,label)
y_pre = rfr.predict(test)

df.loc[df['Age'].isnull(),'Age']=y_pre
```

##### KNNImputer

```python
import numpy as np
from sklearn.impute import KNNImputer
nan = np.nan
X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]
imputer = KNNImputer(n_neighbors=2, weights="uniform")
imputer.fit_transform(X)
```



### 缺失有意义

```python
#缺失值二值化，掩码处理
#即nan的地方为ture，其他为false
df1 = pd.isna(df)
```

```python
#给缺失值赋值
给缺失值 赋值 一个不存在的值，比如 -1 或者 ‘nan’  。
赋值之后可以进行编码处理。
```



# 查看异常值

箱形图

上四分位数（Q3），中位数，下四分位数（Q1） ，四分位数差（IQR，interquartile range）Q3-Q1 

```python
#适用于数值型特征
for i in df:
    if df[i].dtypes!='object':
        print("*******************",i,"***********************")
        plt.figure()
        sns.boxplot(x=i,data=df)
        plt.show()
```



describe

```python
#数值型#类别型
for i in df:
    if df[i].dtypes!='object':
        print("*******************",i,"***********************")
        print(df[i].describe().apply(lambda x: format(x, 'f')))
        print()
    else:
        print(df[i].describe())
        print()    
```



# 异常值处理

异常值大部分情况下都可以视为缺失值，按缺失值处理：填充，删除，不处理，视为有意义。

大部分情况采用填充策略。采用的模型对异常值不敏感时不处理，如树模型对极端数值不敏感。



### 替换填充

```python
df[col] = df[col].replace({365243: np.nan})
```



### 上下限删除

箱型图删除

```python
#定义一个上下限
lower = data['月销量'].quantile(0.25)-1.5*(data['月销量'].quantile(0.75)-data['月销量'].quantile(0.25))
upper = data['月销量'].quantile(0.25)+1.5*(data['月销量'].quantile(0.75)-data['月销量'].quantile(0.25))

#过滤掉异常数据
data['qutlier'] = (data['月销量'] < lower) | (data['月销量'] > upper) 
qutlier_data=data[data['qutlier'] ==False]
```



3σ原则删除

如果数据符合正态分布，可以用3σ原则删除数据。

```python
data['three_sigma'] = data['月销量'].transform( lambda x: (x.mean()-3*x.std()>x)|(x.mean()+3*x.std()<x))
data[data['three_sigma']==True]
correct_data=data[data['three_sigma']==False]
```



# 数值化

类别型变量变为数值型变量



### 一般处理

LableEncode			有序特征（如1级、2级）		类别型变量、树模型

OnehotEncode		无序特征（如 红黄绿）			类别型变量、离散型数值变量、线性模型、神经网络



##### LabelEncoder

```python
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
app_train[col] = le.fit_transform(app_train[col])

le.inverse_transform(label)		#逆转
le.classes_						#类别数
```



##### OneHotEncoder

```python
from sklearn.preprocessing import OneHotEncoder

# dtype=np.float64，指定onehot列类型。
# sparse=True 输出稀疏的格式。
# handle_unknown='ignore'，忽略没fit到的值，编码00000
onehot = OneHotEncoder(sparse=False,handle_unknown='ignore')
onehot.fit_transform(df)

# categories='auto' ,特征有几个值，就会变成多少列。
# categories=[feature0_list,feature1_list]，每个list中有几个值，对应的特征就会变成多少列。
# 这个参数不好用，最好不要用。
brand_level_list = np.sort([100,200]) #这个list中的数值必须为对应特征里有的数值，在这里指定特征里的哪个数值，就编码哪个数值，其他都是000
cate_one_id_list = np.sort([100,200])
categories_list = [brand_level_list,cate_one_id_list]
onehot = OneHotEncoder(categories=categories_list)

#drop=None, drop='first',drop='if_binary' ?

onehot = OneHotEncoder(sparse=False,handle_unknown='ignore')
train_obj_onehot = pd.DataFrame(onehot.fit_transform(train_obj),columns=onehot.get_feature_names())
#onehot后的数据集和原数据集的合并可以使用 sklearn.compose.ColumnTransformer
```



##### dummy

也称为哑编码。

该方法是快捷的 onehot编码。有一个特征值被编码为000，可以节省一列

```
train = pd.get_dummies(train)
```



### 高基数特征

如果特征是无序的，比如256种颜色，不适用于labelencoder，onehot特征又太多。

可以直接使用特征哈希、bin-counting、meanencoder。或者onehot+pca、labelencoder+大数。embedding 暂时不写在这里。

![1575356771847](picture/1575356771847-1603441279224.png)



##### onehot+pca

如果onehot 后特征太多，采用这种方法



##### labelencoder+大数

比如高基数特征有100个，你编码后加上 10万 ，可以 在保证不损失精度的条件下归一化



##### 特征哈希

把大量类别特征映射到固定维度m特征空间中

m<之前维度相当于把一些特征按**字典序**看成了一个，m=之前维度相当于onehot，会节约空间

```python
import pandas as pd

df = pd.DataFrame(js)

m = len(df.business_id.unique())

from sklearn.feature_extraction import FeatureHasher
h = FeatureHasher(n_features=m, input_type='string')
f = h.transform(review_df['business_id']).toarray()
```



##### Bin-counting

https://mp.weixin.qq.com/s/zp13KQxpSra8SWxTLkunCQ

适用与树模型，这个方法还没看懂。

编码为一个实际值介于 0 和 1 之间的特征，用 该值下目标变量的多个条件概率 （当 x 为一个值时y 有多大可能为0，1）来代替一个特征

```python
#区间计数

#读数据
import pandas as pd
df = pd.read_csv('ctr_data.csv')

#从原始数据中构造统计特征
def click_counting(x, bin_column):
    #计算不同label的 每个分箱变量各有多少个
    clicks = pd.Series(x[x['click'] > 0][bin_column].value_counts(), name='clicks')
    no_clicks = pd.Series(x[x['click'] < 1][bin_column].value_counts(), name='no_clicks')

    counts = pd.DataFrame([clicks, no_clicks]).T.fillna('0')
    print(counts)
    counts['total'] = counts['clicks'].astype('int64') + counts['no_clicks'].astype('int64')
    #counts 是 id（分箱变量），点击次数，不点击次数，总点击次数的表
    return counts
#构造出概率特征
def bin_counting(counts):
    counts['N+'] = counts['clicks'].astype('int64').divide(counts['total'].astype('int64'))
    counts['N-'] = counts['no_clicks'].astype('int64').divide(counts['total'].astype('int64'))
    counts['log_N+'] = counts['N+'].divide(counts['N-'])

    #如果上面结果波动过大，可以考虑将其对数化
    
    bin_counts = counts.filter(items=['N+', 'N-', 'log_N+'])
    return counts, bin_counts

#运行
bin_column = 'device_id'
device_clicks = click_counting(df.filter(items= [bin_column, 'click']), bin_column)
device_all, device_bin_counts = bin_counting(device_clicks)

#查看结果 ，可用 N+  N-  log_N+ 来代替新特征，log_N+比较好用，多分类时直接N+除1-N+
print(device_all.sort_values(by = 'total', ascending=False).head(4))
```



##### MeanEncoder

```python
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import KFold
from itertools import product
import pandas as pd
# https://zhuanlan.zhihu.com/p/26308272

class MeanEncoder:
    def __init__(self, categorical_features, n_splits=5, target_type='classification', prior_weight_func=None):
        """
        :param categorical_features: list of str, the name of the categorical columns to encode

        :param n_splits: the number of splits used in mean encoding

        :param target_type: str, 'regression' or 'classification'

        :param prior_weight_func:
        a function that takes in the number of observations, and outputs prior weight
        when a dict is passed, the default exponential decay function will be used:
        k: the number of observations needed for the posterior to be weighted equally as the prior
        f: larger f --> smaller slope
        """

        self.categorical_features = categorical_features
        self.n_splits = n_splits
        self.learned_stats = {}

        if target_type == 'classification':
            self.target_type = target_type
            self.target_values = []
        else:
            self.target_type = 'regression'
            self.target_values = None

        if isinstance(prior_weight_func, dict):
            self.prior_weight_func = eval('lambda x: 1 / (1 + np.exp((x - k) / f))', dict(prior_weight_func, np=np))
        elif callable(prior_weight_func):
            self.prior_weight_func = prior_weight_func
        else:
            self.prior_weight_func = lambda x: 1 / (1 + np.exp((x - 2) / 1))

    @staticmethod
    def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func):
        X_train = X_train[[variable]].copy()
        X_test = X_test[[variable]].copy()

        if target is not None:
            nf_name = '{}_pred_{}'.format(variable, target)
            X_train['pred_temp'] = (y_train == target).astype(int)  # classification
        else:
            nf_name = '{}_pred'.format(variable)
            X_train['pred_temp'] = y_train  # regression
        prior = X_train['pred_temp'].mean()

        col_avg_y = X_train.groupby(by=variable, axis=0).agg(
                                                mean=pd.NamedAgg(column='pred_temp', aggfunc='mean'),
                                                beta=pd.NamedAgg(column='pred_temp', aggfunc='size'))
        col_avg_y['beta'] = prior_weight_func(col_avg_y['beta'])
        col_avg_y[nf_name] = col_avg_y['beta'] * prior + (1 - col_avg_y['beta']) * col_avg_y['mean']
        col_avg_y.drop(['beta', 'mean'], axis=1, inplace=True)

        nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values
        nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values

        return nf_train, nf_test, prior, col_avg_y

    def fit_transform(self, X, y):
        """
        :param X: pandas DataFrame, n_samples * n_features
        :param y: pandas Series or numpy array, n_samples
        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features
        """
        X_new = X.copy()
        if self.target_type == 'classification':
            skf = StratifiedKFold(self.n_splits)
        else:
            skf = KFold(self.n_splits)

        if self.target_type == 'classification':
            self.target_values = sorted(set(y))
            self.learned_stats = {'{}_pred_{}'.format(variable, target): [] for variable, target in
                                  product(self.categorical_features, self.target_values)}
            for variable, target in product(self.categorical_features, self.target_values):
                nf_name = '{}_pred_{}'.format(variable, target)
                X_new.loc[:, nf_name] = np.nan
                for large_ind, small_ind in skf.split(y, y):
                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(
                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target,
                        self.prior_weight_func)
                    X_new.iloc[small_ind, -1] = nf_small
                    self.learned_stats[nf_name].append((prior, col_avg_y))
        else:
            self.learned_stats = {'{}_pred'.format(variable): [] for variable in self.categorical_features}
            for variable in self.categorical_features:
                nf_name = '{}_pred'.format(variable)
                X_new.loc[:, nf_name] = np.nan
                for large_ind, small_ind in skf.split(y, y):
                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(
                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, None,
                        self.prior_weight_func)
                    X_new.iloc[small_ind, -1] = nf_small
                    self.learned_stats[nf_name].append((prior, col_avg_y))
        return X_new

    def transform(self, X):
        """
        :param X: pandas DataFrame, n_samples * n_features
        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features
        """
        X_new = X.copy()

        if self.target_type == 'classification':
            for variable, target in product(self.categorical_features, self.target_values):
                nf_name = '{}_pred_{}'.format(variable, target)
                X_new[nf_name] = 0
                for prior, col_avg_y in self.learned_stats[nf_name]:
                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[
                        nf_name]
                X_new[nf_name] /= self.n_splits
        else:
            for variable in self.categorical_features:
                nf_name = '{}_pred'.format(variable)
                X_new[nf_name] = 0
                for prior, col_avg_y in self.learned_stats[nf_name]:
                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[
                        nf_name]
                X_new[nf_name] /= self.n_splits

        return X_new
```



# 离散化/分箱

连续数值型特征、需要分箱的特征、时间数据按月或者按年分箱。

方法有等频、等宽、聚类、运算

主要对线性模型提升较大，对树模型影响较小。



### 连续数据的离散化

等宽分箱：将区间等宽分割，可能出现空箱。

等频分箱：保证每个区间数据量一致，可能将原本是相同的两个数值却被分进了不同的区间。



##### KBinsDiscretizer

```python
# strategy：uniform（等宽分箱）、quantile（等频分箱）、kmeans（聚类分箱）
from sklearn.preprocessing import KBinsDiscretizer
est = KBinsDiscretizer(n_bins=3, encode='onehot-dense', strategy='uniform')

df = est.fit_transform(data)
```



##### pd.cut

```python
#等宽分箱
df['Fare'] = pd.cut(df['Fare'], 4, labels=range(4))

#等频分箱
df['Fare'] = pd.qcut(df['Fare'],4,labels=['bad','medium','good','awesome'])

#自定义分箱
bins=[0,200,1000,5000,10000]
df['amount1']=pd.cut(df['amount'],bins)
```



##### KMeans

```python
#对某一列进行聚类就是分箱
from sklearn.cluster import KMeans

x = df['Fare'].values.reshape(-1,1)
kmeans = KMeans(n_clusters=4,random_state=0)
df['Fare2'] = kmeans.fit_predict(x)
```



##### 运算分箱

```python
import numpy as np

small_counts = np.random.randint(0, 100, 20)
#将small_counts除以10并向下取整的结果返回，并不是严格意义上的分箱
np.floor_divide(small_counts, 10)
```



### 连续数据的二值化

```python
#大于阈值为1，小于阈值为0
from sklearn.preprocessing import Binarizer

x = df['Fare'].values.reshape(-1,1)
bir = Binarizer(threshold=df['Fare'].mean())
df['Fare'] = bir.fit_transform(x)
```



### 时间数据的离散化

```python
#这只是一个示例，详细要结合实际和操作时间特征的方法
import pandas as pd

for i single_data in enumerate(df['datetime']):
    single_data_tmp = pd.to_datetime(single_data)
    df['datetime'][i] = single_data_tmp.weekday()
```



### 离散数据的离散化

就是把很多小箱 变成几个大箱

```python
import pandas as pd
data['age'] = pd.DataFrame(['0-10','10-20','0-10'])
df = pd.DataFrame([['0-10','0-40'],['10-20','0-40']],columns=['age','age2'])#自己构造的

data_tmp = pd.merge(df,data,on='age',how='inner')
data = data_tmp.drop('age',axis=1)
```

除了这种方法，先编码再分箱 应该也可以达到相同效果



# 无量纲化

标准化、归一化 、中心化 等概念在翻译时经常被混用，已经不能分清，所以统称为**无量纲化**。



作用：加快速度（梯度和矩阵为核心的算法中，譬如逻辑回归，支持向量机，神经网络）

​			统一量纲，提升精度（在距离类模型，譬如K近邻，K-Means聚类中）  



使用：先试试看**StandardScaler**，效果不好换**MinMaxScaler**。

​			在异常值多，噪声非常大时，我们可能会选用分位数来无量纲化，此时使用**RobustScaler**。

​			在不涉及距离度量、协方差计算？、数据不符合正太分布的时候，**MinMaxScaler**。

​			希望压缩数据，却不影响数据的稀疏性时，我们会使用**MaxAbsScaler？**。

![无量纲化](picture/无量纲化.PNG)



### StandardScaler

适用于有outlier（测试集的数据范围超过或部分超过了训练集）数据的情况，适用于有较多异常值和噪音的情况

可以把正态分布变为标准正态分布，这种方法不改变数据的高斯分布，对非正态分布的数据效果不好

```python
#可以自动处理缺失值
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(data)#fit，本质是生成均值和方差
x_std = scaler.transform(data)

scaler.inverse_transform(x_std)#逆转标准化

#标准化前的均值与方差
print(scaler.mean_)
print(scaler.var_)	
#标准化后的均值与方差
print(x_std.mean())	
print(x_std.std())	
```



### MinMaxScaler

适用于无outlier数据的情况

适用于 1.标准差非常小	2.（稀疏数据）需要保存住０元素

```python
#可以自动处理缺失值
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()		#feature_range=[5,10]，可以控制归一化的范围
scaler = scaler.fit(data)	#fit，在这里本质是生成min(x)和max(x)
result = scaler.transform(data)

scaler.inverse_transform(result)   #将归一化后的结果逆转

scaler = scaler.partial_fit(data)	#数据量大时用这种fit
```

```python
#使用numpy来实现归一化
import numpy as np
X = np.array([[-1, 2], [-0.5, 6], [0, 10], [1, 18]])
#归一化
X_nor = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
#逆转归一化
X_returned = X_nor * (X.max(axis=0) - X.min(axis=0)) + X.min(axis=0)
```



###  Normalization

使用二次型（点积）或者其它核方法计算两个样本之间的相似性之前用

该方法主要应用于文本分类和聚类中。例如，对于两个TF-IDF向量的l2-norm进行点积，就可以得到这两个向量的余弦相似性。

```python
from sklearn import preprocessing
normalizer = preprocessing.Normalizer().fit(X)
normalizer.transform(X)
```



### MaxAbsScaler

设置参数  with_mean=False  ，可以缩放稀疏数据。

```python
from sklearn.preprocessing import MaxAbsScaler
import numpy as np

X_train = np.array([[ 1., -1.,  2.],
                    [ 2.,  0.,  0.],
                    [ 0.,  1., -1.]])

max_abs_scaler = MaxAbsScaler()
X_train_maxabs = max_abs_scaler.fit_transform(X_train)
X_train_maxabs
```



# 特征衍生

自动特征衍生适用多表特征组合。[featuretool.md](picture/featuretool.md)



### PolynomialFeatures

数值型特征相乘，类别型特征拼接

```python
from sklearn.preprocessing import PolynomialFeatures

poly_target = df['click']
poly_features = df[['device_conn_type', 'C1', 'site_domain_pred_1']]

poly_transformer = PolynomialFeatures(degree = 3)
poly_features = poly_transformer.fit_transform(poly_features)

#生成新的DataFrame
poly_features = pd.DataFrame(poly_features, columns = poly_transformer.get_feature_names(['device_conn_type', 'C1', 'site_domain_pred_1']))
poly_features['click'] = poly_target
```



### 基于专家经验构建特征

比如 3个月点击，半年购买等聚合。

特征间进行加减乘除，同比/环比  等数学运算。

从字符冲或者身份证中切割提取特征。

```python
train['CREDIT_INCOME_PERCENT'] = train['AMT_CREDIT'] / train['AMT_INCOME_TOTAL']
```



# 特征选择

特征选择包括 3中类型的方法

1.Filter：根据自变量和目标变量的关联来选择变量。包括方差选择法 和 利用相关性、卡方、互信息等指标进行选择。[Filter各方法原理](./picture/单变量特征选择各方法原理.md)

2.Wrapper：利用模型`coef_` 属性 或 `feature_importances_` 属性获得变量重要性，排除特征，一个模型多次迭代。

3.Embedded：利用模型单独计算出每个特征和Lable的系数，设置阈值，删除特征。多个模型一次训练。

其中 **方差选择法**和 **单变量特征选择** 属于Filter，**递归特征消除**属于 Wrapper，**SelectFromModel**属于Embedded



方差选择法做预处理，然后使用递归特征消除法



### 方差选择法

计算每一列特征的方差，小于给定阈值的特征即删除。方差越小代表该列没有变化，也不能反映y的变化情况。

方差选择法是特征选择的预处理步骤，方差选择后还可以用其他方法继续选择。

```python
#方差计算(s2) = Σ [(xi - x̅)2]/n - 1
from sklearn.feature_selection import VarianceThreshold

X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]
sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
X_new = sel.fit_transform(X)
```



### 单变量特征选择

根据自变量和目标变量的关联来选择变量。包括相关系数、卡方、互信息。

单变量特征选择准确性一般，但相比其他模型选择方法速度更快。不推荐使用。



回归：

```
f_regression：皮尔逊相关系数，只对线性关系敏感。
mutual_info_regression：最大信息系数MIC，线性关系非线性关系都可
```

分类 :

```
f_classif：方差分析
mutual_info_classif：互信息，线性关系非线性关系都可
chi2：卡方检验，要求x尽量离散
```

![1575255200786](picture/1575255200786-1603691572532.png)



SelectBest 只保留 k 个最高分的特征，

SelectPercentile 只保留用户指定百分比的最高得分的特征。

```python
from sklearn.datasets import load_iris

iris = load_iris()
X, y = iris.data, iris.target
print(X.shape)

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

#chi2 可用自定义函数计算：该输入特征矩阵和目标向量，输出二元组（评分，P值）的数组
selector = SelectKBest(chi2, k=2).fit(X, y)

#相似性得分越大越好，pvalue越小越好
print(selector.scores_)
print(selector.pvalues_)
print(selector.get_support(True))

X_new = selector.transform(X)
print(X_new.shape)
```

注：稀疏格式数据只能使用卡方检验和互信息法



### 递归特征消除法

Recursive Feature Elimination （RFE）通过考虑越来越小的特征集合来递归的选择特征。

通过`coef_` 属性 或者 `feature_importances_` 属性获得重要性，删除特征

```python
from sklearn.datasets import load_digits
digits = load_digits()
X = digits.images.reshape((len(digits.images), -1))
y = digits.target

from sklearn.svm import SVC
from sklearn.feature_selection import RFE
svc = SVC(kernel="linear", C=1)
rfe = RFE(estimator=svc, n_features_to_select=1, step=1) #n_features_to_select：剩几个，step：每次删几个
df = rfe.fit_transform(X, y)
```



RFECV ：每次选择不同列进行RFE 完成交叉验证，找到最优特征。

```python
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.model_selection import StratifiedKFold
from sklearn.feature_selection import RFECV
from sklearn.datasets import make_classification
 
X, y = make_classification(n_samples=1000, n_features=25, n_informative=3,
                           n_redundant=2, n_repeated=0, n_classes=8,
                           n_clusters_per_class=1, random_state=0)

svc = SVC(kernel="linear")
rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),scoring='accuracy')

rfecv.fit(X, y)
x = rfecv.transform(X)

print("Optimal number of features : %d" % rfecv.n_features_)
print("Ranking of features : %s" % rfecv.ranking_)
print("Support is %s" % rfecv.support_)
print("Grid Scores %s" % rfecv.grid_scores_)
```



### SelectFromModel

`SelectFromModel` 是一个 meta-transformer（元转换器） 。

它可以用来处理任何带有 `coef_` 或者 `feature_importances_` 属性的训练之后的评估器。 

如果相关的`coef_` 或者 `feature_importances_`  属性值低于预先设置的阈值，这些特征将会被认为不重要并且移除掉。

除了指定数值上的阈值之外，还可以通过给定字符串参数来使用内置的启发式方法找到一个合适的阈值。



基于树的特征选择

```python
# 树模型可以找到非线性关系，但要控制树的深度来避免过拟合问题。还有要运用交叉验证保证每次的分数最好。
# 随机森林效果好，但是有两个问题：重要特征没被选择出来，对类别多的变量有利

from sklearn.datasets import load_iris
iris = load_iris()
X, y = iris.data, iris.target

from sklearn.ensemble import ExtraTreesClassifier
from sklearn.feature_selection import SelectFromModel
clf = ExtraTreesClassifier()
clf = clf.fit(X, y)
model = SelectFromModel(clf, prefit=True)
X_new = model.fit_transform(X)
```

基于 L1 的特征选择

```python
#L1降维的原理在于保留多个对目标值具有同等相关性的特征中的一个，所以没选到的特征不代表不重要

from sklearn.datasets import load_iris
ris = load_iris()
X, y = iris.data, iris.target

#可用于此目的的稀疏评估器有回归的 `linear_model.Lasso`, 以及分类的 `linear_model.LogisticRegression` 和 `svm.LinearSVC`
from sklearn.svm import LinearSVC
from sklearn.feature_selection import SelectFromModel
lsvc = LinearSVC(C=0.01, penalty="l1", dual=False).fit(X, y) #参数 C 控制着稀疏性，C越小特征越少
model = SelectFromModel(lsvc, prefit=True)
X_new = model.transform(X)
```

稳定性选择

```python
#同L1和树模型相比，好的特征不会快速下降，得分1代表好特征。

from sklearn.linear_model import RandomizedLasso
from sklearn.datasets import load_boston

boston = load_boston()
X = boston["data"]
Y = boston["target"]
names = boston["feature_names"]

rlasso = RandomizedLasso()
rlasso.fit(X, Y)

print("Features sorted by their score:")
print(sorted(zip(map(lambda x: round(x, 4), rlasso.scores_), names), reverse=True))
```



### 基于变量聚类的特征选择

```python
#给变量进行聚类，每类中有若干个变量
from varclushi import VarClusHi
vc=VarClusHi(trainData[var_list],maxeigval2=0.6,maxclus=None)

#N_Vars变量数量,Eigval1方差
print(vc.info)

#查看分类结果
vc_list=vc.rsquare.reset_index()
vc_list=pd.DataFrame(vc_list,columns=['Cluster','Variable','RS_Own'])
print(vc_list)

#随机森林训练重要性
X = trainData[var_list]
y = trainData[target]

from sklearn.ensemble import RandomForestClassifier
RFC = RandomForestClassifier(random_state=42)
RFC_Model = RFC.fit(X,y)

importances = []
for i in vc_list['Variable']:
	importances.append(RFC_Model.feature_importances_[i])
vc_list['importances'] = importances

#根据聚类结果及重要性进行筛选
vc_list = vc_list.groupby('Cluster').apply(lambda x:x[x['importances']==x['importances'].max()])
var_list = list(vc_list[vc_list['Importances'] >= 0.0001].Variable)
```



# 降维算法

特征选择之后，仍有大量特征，可选择降维算法



# 自定义特征处理

如果不使用函数，而使用  lambda  表达式，transformer  不能被  pickle

```python
import numpy as np
from sklearn.preprocessing import FunctionTransformer
transformer = FunctionTransformer(np.log1p)
X = np.array([[0, 1], [2, 3]])
transformer.transform(X)
```



# magic Silver Bullet

### category频度统计

对GBDT十分有效。原理：干掉刺头，长尾的值全部变为他的频度，使学习难度降低。

```python
df_counts = df['区域'].value_counts().reset_index()
df_counts.columns = ['区域','区域频度统计']
df = df.merge(df_counts,on =['区域'],how='left')
```

































































