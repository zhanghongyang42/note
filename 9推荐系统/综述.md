策略 - 模型 -策略 -业务



# 机器学习基础



## 正则化

目的：减小参数大小或数量，防止过拟合。



loss_with_regularization = loss + λf(w)

*正则化项永远是非负的。所以会使之前的参数变小。*

*所以L1是w的绝对值，L2是w的平方*



L2正则：

![image-20211114225805310](picture/image-20211114225805310.png)

![image-20211114225946938](picture/image-20211114225946938.png)

特点：w会变小，但不为0，不会形成稀疏解



L1正则：

![image-20211114231324158](picture/image-20211114231324158.png)

L1可以形成稀疏解的原因：

画出损失函数和正则项的图。L1是菱形，L2是圆形，损失函数与菱形更易相交坐标轴，即一些w为0



## 最优化方法

优化方法是对损失函数进行优化。

泰勒级数是牛顿法和梯度法基础。

为什么梯度方向最快：通过泰勒展开求。



## 贝叶斯公式

![image-20211115214007956](picture/image-20211115214007956.png)



# 推荐架构

请求--召回--排序--展示



内容源的收集：爬虫--内容库--关键词--人工库--人工--推荐库

日志的收集：客户端--埋点--数据库--kafka--推荐组

宽表数据：hive--spark/sklearn--hdfs



模型训练：

​					全量手动更新

​					增量更新（天/小时）

模型预测：

uid--推荐引擎--解析user属性--召回模块（多路异步离线召回）--写入数据库--排序--结果--业务规则--展示

















# 内容画像

分类，关键词，时效，规范



内容源--爬取--库--**算法接口**--人工校正



文本分类：一级、二级

主题提取：LDA

关键词抽取：算法抽取，词典

关键词抽取算法抽取：tf-idf，计算权重，取top15



id	categorie	keyword	LDA编号	过期时间	是否违规



内容画像的用途：根据关键词召回，用作排序特征。



## TF-IDF

要先有一个语料库，是计算某篇文章关键词权重的算法。也可以用于惩罚热门物品。

TF-IDF的前提是词袋模型 ，比词袋模型减少了无用词的影响。

实现为CountVectorizer+TfidfTransformer   或者  TfidfVectorizer。



### 词袋模型

词袋模型就是 忽略词序语法近义等，每个词相互独立，简单统计每篇文章token的次数。

词集模型就是 忽略词序语法近义等，每个词相互独立，简单统计每篇文章token是否出现。

单纯的词集，词袋模型一般要转换成onehot的形式使用。

词袋模型一般用于关键词权重计算。词集模型用的不多，猜测用于有监督模型中，代码见sklearn。

```python
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()

corpus = [
    'This is the first document.',
    'This is the second second document.',
    'And the third one.',
    'Is this the first document?',
]

X = vectorizer.fit_transform(corpus)
print(X.toarray())
print(vectorizer.get_feature_names())

print(vectorizer.transform(['Something completely new.']).toarray())

#ngram_range默认1个词为1个token，可以设置1，2，3个词都是1个token，这样可以把词的顺序信息训练进来
bigram_vectorizer = CountVectorizer(ngram_range=(1, 3))
X_2 = bigram_vectorizer.fit_transform(corpus).toarray()

print(X_2)
print(bigram_vectorizer.get_feature_names())
```



### TfidfTransformer

TF 词频

![20180806135836378](picture/20180806135836378.png)



IDF 逆文档频率 ：即词频的权重

![20180806140023860](picture/20180806140023860.png)

TF-IDF = TF*IDF

```python
from sklearn.feature_extraction.text import TfidfTransformer
transformer = TfidfTransformer(smooth_idf=False)

#一个列表代表一篇文章，共有6篇文章
#每一列代表一个词，3代表第一个词在第一篇文章里出现了3次。
#每一篇文章的每一个词都可以计算出tf-idf，最后把结果归一化
counts = [[3, 0, 1],
          [2, 0, 0],
          [3, 0, 0],
          [4, 0, 0],
          [3, 2, 0],
          [3, 0, 2]]

tfidf = transformer.fit_transform(counts)
print(tfidf.toarray())
```



## BP算法





# CTR预估

手动交叉

深度学习交叉

注意力机制





















# 冷启动

规则

热度

商品内容











ABTEST

数据

运营

多指标优化，staytime

topk的召回































